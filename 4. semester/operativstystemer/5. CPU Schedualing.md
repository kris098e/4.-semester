![[ch05-new.pdf]]schedualing algortihms
* Shortest job first, try to compute the cpu burst for prediction
	* exponential averaging with variable $$\alpha$$ to manage how much the previous bust has of impact on the prediction

multiple priorities, different ques with different priorities and time slots. With multiprocessor systems (multi core), there are different schedualing algorithms such that processes are schedualed into a que for the different processes or all processes are in same que and assigned to a core. A process may want to be ran on the same CPU it has just ran on, as the cache may have something belonging to it. 
as processes has priority, this may result in **starvation** for some processes, and the solution is aging, where a processes priority increases the more time it has waited. 

# Keywords
* preemptive schedualing
* nonpreemptive schedualing (processes cannot be paused or taken of CPU, without the process itself calling for it)
* schedualing criterias
	* wait time (wait time for a process)
	* responsiveness (small enough time quantums)
	* ?
* priority schedualing (linux has 140 priority levels. Default is 120 _if seen on "top" 20=120_, can use "**nice -15**" to bump a priority to 120+15. can also do "sudo **nice --15**" then the process will get 120-15=105 as PR(priority))
	* starvation (if there are always processes with higher priority, then some processes wont get to execute)
	* aging (priority of process increases as it gets older)
* Symmetric multiprocessing: every cpu are the same
* asymmetric multiprocessing: one cpu controls what goes on the other cpus
* multilevel feedback ques
* NUMA (non uniform memory access, better to have the same process on the same CPU instead of migrating to other CPU, fewer migrations is better for execution time)
* CFS Completly Fair Schedualer
* 


# real time schedualing
## Keywords
* soft real time schedualing (try their best to be real-time)
* hard real time schedualing (not seen implemented?)
* Rate monotonic schedualing 
	* sometimes doesnt work for load lower than 100% (see slides)
		* earlist deadline first schedualing can fix this 
* 

# book
* Cpu schedualer is the one deciding which process gets the next spot
* Dispatcher is the one context switching the contents on the cpu. I.e puts the new program block to the cpu to execute from
* preemtive, nonpreemtive schedualing
* schedualing
	* first come first served
	* shortest job first (nonpreemptive), shortest remaining job first (preemptive)
	* round robbin (time quantum and wrap arounds)


---
Title: OS Video
Date: 27-02-2023
Time: 20:12
---

> [!warning]
> CPU scheduling should be called process scheduling because we schedule the processes. 
> > [!warning] 
> > On modern operating systems it is kernel-level threads—not processes—that are in fact being scheduled by the operating system. However, the terms "process scheduling" and "thread scheduling" are often used interchangeably

### Whats the difference between Exponential and Hyberexponential?
> [!Info] Know the difference
> **Hyberexponential**: Different rate
> **Exponential**: Same Rate

### CPU and IO bound proceses
short burst of CPU and then IO then CPU then IO  bursts. 

### Whats the four circumstances that can make a CPU switch process
1. switches from running to waiting state
	- for example, as the result of an I/O request or an invocation of wait() for the termination of a child process)
2. switches from running to ready state
	- for example, when an interrupt occurs
3. Switches from waiting to ready
	- for example, at completion of I/O
4. Terminates

### What is the difference between nonpreemptive and preemptive scheduling?
#### nonpreemptive
-   The operating system cannot interrupt a running task, and the running task must voluntarily give up control and yield to the next task in line.
-   The running task must complete its execution before the next task can start.
-   Simpler and more predictable than preemptive scheduling algorithms.
-   Can lead to longer response times and decreased overall system performance if a task monopolizes the CPU and does not yield control.
- Situation 1 and 4 in [[CPU schedulingen#Whats the four circumstances that can make a CPU switch process | The four circumstances]]

#### preemptive
-   The operating system can interrupt a running task and switch to another task at any time.
-   The running task may not have completed its execution before being interrupted.
-   Provides better responsiveness as the operating system can quickly switch to more important or urgent tasks.
-   Can lead to overhead and decreased efficiency if tasks are constantly being interrupted.
- All other scheduling is preemptive e.g 
	- access to shared data
	- Preemption while in kernel mode
	- Interrupts occurring during crucial OS activities.
- Used by Windows, macOS, Linux and UNIX.

### Whats the role of the dispatcher?
- Switching context from one process to another
- Switching to user moder
- Jumping to the proper location in the user program to resume that program
#### Illustration
![[Pasted image 20230319121217.png]]


### What is dispatch latency?
Time it take for the dispatcher to stop one process and start another running

can use `cat /proc/x/status` to retrieve information about how many voluntary context switches the process has made and nonvoluntary. 

### When does Voluntary and non-voluntary context switch happen
**Voluntary:** When a process self give up the control of CPU
**Non-Voluntary:** When a process get forced to give up the control e.g interrupt


### Name some criteria to compare CPU-scheduling algorithm
#### CPU utilization
- Measure how busy the CPU are 
- Range from $0$ to $100$
- Real time system: $40\%$ (lightly loaded system) to $90\%$ (Heavy loaded systems)
- top command on Linux, MacOS, and UNIX
#### Throughput
- The number of processes that are completed per time unit
- For a **long** process maybe one over several seconds
- For **short** process may be tens of processes per seconds.
#### Turnaround time
- The point of view from a process
- The interval from submission to execution
- Sum of periods spent in the ready queue, executing and doing I/O
#### Waiting time
- How the the process wait in the waiting queue
#### Response time
- How long before a process gives first output.
- **NOT** necessary finish.

> [!info] What do we want?
> > [!done] Maxmize
> CPU utilzation
> throughput
> 
> > [!danger] Minimize
> Turnaround time
> Waiting time
> response time 

## Schedualing algortihms
choose which processes are to run on the CPU's core

### What is FCFS?
- Stands for First- come, first-served
- Scheduling algorithm where it "Først til mølle princippet"
average waiting time often quite long (think of long process first but 2 short are also queued)

If having CPU hugging process and multiple I/O, then when I/O devices finishes they have to wait for the CPU and the I/O sits idle

### What is SJF 
- Stands for Shortes-Job-First
- Scheduling algorithm where it is the CPU with the shortest Burst time that go first.
provably the most optimal for waiting time, since shortest jobs are schedualed first

cannot be implemented, however we can try to predict the next process' CPU based on the formula , meaning it based on the previous burst-times
τ n+1 = α t n + (1 − α)τ n .

The value of t n contains our most recent information, while τ n stores the past
history. The parameter α controls the relative weight of recent and past history
in our prediction. If α = 0, then τ n+1 = τ n , and recent history has no effect
(current conditions are assumed to be transient). If α = 1, then τ n+1 = t n , and
only the most recent CPU burst matters (history is assumed to be old and
irrelevant). More commonly, α = 1/2, so recent history and past history are
equally weighted. The initial τ 0 can be defined as a constant or as an overall
system average. Figure 5.4 shows an exponential average with α = 1/2 and
τ 0 = 10.
![[Pasted image 20230530084622.png]]

**if preemptive:** called _shortest-remaining-time-first_, since if a new process arrives we would rather scheduale this if the process on the CPU has longer time than this.

### How can we define the exponential average?
$T_n=$ The length of the $n$th CPU burst
$\tau_{n+1}=$ Predicted value for the next CPU burst
$\alpha=$ The relative weight of recent and past history
$$
\tau_{n+1}=\alpha t_n+(1-\alpha) \tau_n .
$$
$\alpha = 1$ Then $T_{n+1}= t_{n}$ Only the most recent CPU burst matters
$\alpha = 0$ Then $T_{n+1}= \tau_{n}$ Recent history has no effect
Commonly: $\alpha= \frac{1}{2}$

### What is a time quantum:
- The amount of time a process get on the CPU
- Typically between $10$ and $100$ milliseconds 

### Describe Round Robin  
-  We have a given time quantum $n$
- Every process get $n$ time on CPU and when we switch
The average waiting time is often long.

#### If short or long quantums
long: mimics FCFS
short: many context switches which will result in overhead. However the typical context switching time is as little as **10 microseconds.**

### Describe Priority Scheduling
- Every process is given a priority
- The process with relative highest priority get on the CPU 
This priority can be set based on a lot of factors. Fx the most paying costumer, how much memory the process uses, how big the CPU-burst is ...

**preemptive** if the process running on the CPU has lower priority than that of an arriving process, preempt.
**nonpreemptive** just put the process on the CPU with highest priority

### What is the starvation problem and the solution:
**Problem:** Low priority processes may never execute
**Solution:**  **Aging**: Give processes that wait higher priority for every time they wait.

### modifying Round Robin
can use priority scheduling with round robin

### What is multilevel Queue?
- Allow us to use different scheduling algorithm
- Common division between **foreground** process(interactive) and **background** process (batch)
#### Example:
1.  Real-time processes (strict deadlines)
2.  System processes  (memory, disk and network management)
3.  Interactive processes (User interaction)
4.  Batch processes (Backups)
- Each queue has absolute priority over lower-priority queues.

### Whats is multilevel feedback queue scheduling?
- Allows a process to move between queues
- Idea is to separate processes according to the characteristics of their CPU bursts
- If the process use to much, it get lower priority
- If don't get any time it get higher priority
![[Pasted image 20230530091057.png]]

#### Multilevel Feedback queue scheduling
Each queue may also have its own scheduling algorithm

An approach is to give fx batch processes 20% of the CPU time and real-time 80%, such that all will progress, but there are still some priorities.

##### Defined by the following:
-   The number of queues
-   The scheduling algorithm for each queue
-   The method used to determine when to upgrade a process to a higher- priority queue
-   The method used to determine when to demote a process to a lower- priority queue
-   The method used to determine which queue a process will enter when that process needs service

Fx can use 3 levels, CPU burst of 8, 16, > 16. Then the CPU burst with lower than 8 will just complete and fx go onto I/O making sure I/O is also fully utilized.

Complex to design, but very flexible.

### What is used in most systems?
Round Robin with multilevel feedback queues? 

## Thread Scheduling
It is not the user-level threads which are schedualed, it is the kernel-level threads that these are mapped to. Usually the thread libraries does not allow the programmer to change the priority of these threads however some does.

### Pthread Scheduling
Can set how you want the process to be schedualed. User-defined or system-defined.

## Multi-processor Scheduling
Different architectures
• Multicore CPU s
• Multithreaded cores
• NUMA systems
• Heterogeneous multiprocessing

### Approaches to Multiple-Processor Scheduling
**Assymmetric multiprocessing** one processor is the master-server and all I/O and schedualing decisions is handled by this one while the others does all the code.
This processor is bottleneck

**Symmetric multiprocessing** all cores can scheduale for themselves. Two approaches for the queues.
1. All threads may be in a common ready queue. (race-conditions, bottleneck on the lock)
2. Each processor may have its own private queue of threads. (used in most systems, also efficient due to cache on the CPU storing data for the threads)

### Multicore processors
Each core has multiple CPU's where each core itself is in charge of its environment. More efficient compared to splitting up such that each core does not have multiple CPU's

As the CPU's are faster than the memory, a solution is that each core gets two or more **Hardware-threads** such that if one stalls, the core can switch to another thread.

Each hardware-thread can run a software thread. If a core has two hardware threads, it is seen as the system has 2x the amount of physical cores and logical cores.

**hyperthreading** is the term used when a physical core seems to be more than just 1 logical core since it has more hardware threads.

two ways of hyper-threading
**coarse-grained** each time a hardware thread stalls and the core must switch to another hardware thread, the instruction pipeline must be flushed, which will take some time. But this is done and the new thread will then fill up the pipeline again

**fine-grained** switch between threads at the boundary of an instruction cycle. As a result switching between threads is small

A core can only execute one hardware-thread at a time
![[Pasted image 20230530094947.png]]

### Load Balancing
attempts to keep the workload evenly distributed across all processors in
an SMP system. Only needed when each processor has its own ready-queue. I.e needs to split the work onto the other cores, such that all cores are used equally
**Push migration** a specific task periodicly checks if a core is not working and pushes tasks from another core
**pull migration** idle processore pulls a waiting task from busy processor.


#### Whats the difference between Soft affinity and Hard affinity?
The load balancer will try to balance the load, what happens if a thread is pushed or pulled to another processor
**Soft affinity:** The operating system attempts to keep a thread running on the same processor, but no guarantees
**Hard affinity:** Allows a process to specify a set of processors it may run on

In NUMA the load balancer should also beware of where the memory for this process lives , such that it scheduales the thread onto where the CPU is closest

there is a natural tension between load balancing and minimizing memory access times

### Heterogeneous Multiprocessing
Each processor may not have the capability of some of the others
Some cores may consume more power and runs faster, can choose which to run on.

Used in ARM **Big.LITTLE** big cores consume more power, little cores less energy and can therefore run for longer periods. Background task on little, preserve energy. 

## Real-Time CPU Scheduling

### What is difference between soft and hard real-time systems 
**Soft real-time systems:** Critical real-time tasks have the highest priority, but no guarantee as to when task will be scheduled.
**Hard real-time systems:** Task must be serviced by its deadline

### Minimizing Latency 
We refer to
event latency as the amount of time that elapses from when an event occurs to
when it is serviced

#### Interrupt Latency 
Interrupt latency refers to the period of time from the arrival of an interrupt
at the CPU to the start of the routine that services the interrupt

One important factor contributing to interrupt latency is the amount
of time interrupts may be disabled while kernel data structures are being
updated. Real-time operating systems require that interrupts be disabled for
only very short periods of time.

#### Dispatch Latency
The amount of time required for the scheduling dispatcher to stop one
process and start another is known as dispatch latency

### Priority-based scheduling
Must be able to preempt and give priority to the tasks, they need the highest highest priority when real-time task
This only make the ability to meet soft-real time systems, need to ensure a deadline is met when implementing hard-real time.

Schedules the processes in **periodic**-fashion for hard-real time. Meaning that the OS will ensure it will run for this amount of time in a given time-frame. If this cant be met it is rejected.

#### Rate-monotonic scheduling
The rate-monotonic scheduling algorithm schedules periodic tasks using a
static priority policy with preemption. If a lower-priority process is run-
ning and a higher-priority process becomes available to run, it will preempt
the lower-priority process. Upon entering the system, each periodic task is
assigned a priority inversely based on its period. The shorter the period, the
higher the priority; the longer the period, the lower the priority.

![[Pasted image 20230530102533.png]]

If this algorithm cannot serve the tasks such that they meet their deadline, it is  not guaranteed that it can  served by any other algorithm

![[Pasted image 20230530102718.png]]
even tho the CPU utilization is 0.94 it cannot be served. P1 will preempt P2. With two processes the CPU utilization is bounded at about 83%. When the amount of tasks reaches infinity it goes down it 69%. Uses the formula 

$N(2^{\frac{1}{N}}-1)$

#### Earliest-Deadline-First Scheduling
![[Pasted image 20230530103321.png]]
Proven to be optimal. In practice, however, it is impossible to achieve this level of CPU utilization due to the cost of context switching between processes and interrupt handling.

#### Proportional Share Scheduling
schedulers operate by allocating T shares among all appli-
cations. An application can receive N shares of time, thus ensuring that the
application will have N∕T of the total processor time. As an example, assume
that a total of T = 100 shares is to be divided among three processes, A, B, and
C. A is assigned 50 shares, B is assigned 15 shares, and C is assigned 20 shares.
This scheme ensures that A will have 50 percent of total processor time, B will
have 15 percent, and C will have 20 percent.

#### POSIX real-time scheduling
Threads can have the scope
+ SCHED FIFO
+ SCHED RR

# Operating-System Examples
## Linux Scheduling
Prior versions had a bad scheduling algorithm for multiprocessors, however they introduced **Completely Fair Schedular**, which is the default

Involves using two scheduling classes
Standard Linux kernels implement
two scheduling classes: (1) a default scheduling class using the CFS scheduling
algorithm and (2) a real-time scheduling class

Uses **targeted latency** meaning that all processes has to be run atleast once in a given time-frame. The task to put on the CPU is done by looking at the task with the lowest virtual run time. I.e if using 200 physical run time miliseconds and you have a nice value of 0, then you will get virtual runtime of 200 miliseconds, if having higher priority you will get less that 200 virtual runtime, if lower priority you will have more than 200 virtual runtime.

Real-time tasks are assigned static priorities within
the range of 0 to 99, and normal tasks are assigned priorities from 100 to 139.
where a value of −20 maps to priority 100 and a nice value of +19 maps to 139.

It is also NUMA-aware ensuring that threads are schedualed on the preferred core. This is done by grouping the cores into domains
![[Pasted image 20230530110008.png]]
CFS would load balance based on the different NUMA nodes and then when in this level, the next level is to chose the correct domain for this thread to run.

## Windows Scheduling
Uses a preemptive scheduling algorithm, works just like priority based round robin.
Uses 32-level priority scheme
has two classes. The **variable class**
contains threads having priorities from 1 to 15, and the **real-time class** contains
threads with priorities ranging from 16 to 31

Upper priority
+ IDLE PRIORITY CLASS
+ BELOW NORMAL PRIORITY CLASS
+ NORMAL PRIORITY CLASS
+ ABOVE NORMAL PRIORITY CLASS
+ HIGH PRIORITY CLASS
+ REALTIME PRIORITY CLASS

within the upper priority level it can have a relative priority

+ IDLE
+ LOWEST
+ BELOW NORMAL
+ NORMAL
+ ABOVE NORMAL
+ HIGHEST
+ TIME CRITICAL
![[Pasted image 20230530110834.png]]
When a variable priority's time quantum expires it is lowered. When a variable priority is realeased from `wait()` its priority its boosted by the dispatcher. Based on the operation the thread is waiting for it gets increased or decreased priority. Fx if the thread waits for keyboard stroke it is given high priority, if it was waiting for a disc operation it gets low boost. This tend to give good responsiveness. High priority is also given fx if the user is interracting with the window the thread manages

Windows has a rule for the `NORAML_PRIORITY_CLASS`
Windows distin-
guishes between the foreground process that is currently selected on the screen
and the background processes that are not currently selected. When a process
moves into the foreground, Windows increases the scheduling quantum by
some factor—typically by 3

Provides good support multiprocessing ensuring no memory penalty is introduced. Groups the cores into groups, fx 1 core has the two logical cores and the thread has information of which it previously ran on.

The load balancer simply increments a number, schedualing a new process on the next core.

# Algorithm Evaluation
Want to focus an specific criteria, could fx be CPU utilization or wait-time

## Deterministic Modeling
Give an example of workload, i.e 5 processes and their CPU-burst times, then runs round-robbin, SJF and FCFS and give out a number for each. Fx can track the average waiting time, which will give out a number, and can then compare all of the methods based on the output of this number. However this means that we only know what to expect when giving this exact input.

## Queueing Models

## Simulations
Simulate a real system using random number generators for how many processes and each CPU burst times ... Can then gather intel from this. However it may be inaccurate since it does not take all the events into account on the real system. 
To battle this, we can make a trace file which monitors a real system, and can then feed this to a simulation
![[Pasted image 20230530123412.png]]
However the trace files may require a lot of memory, and the simulator can require a lot of code and the debugging of the simulator may be a major task

## Implementation
Implementing the testing into an actual OS, which may cost a lot of time. However, some may try to circumvent the process scheduler, if it favors interractive tasks, then the coder can just switch into interative mode. This means that it is hard to test all cases, for how it will work for all systems.

May be able to implement such that the user can himself change the scheduling algorithm, another approach is to use the API such that the threads has different priority based on what the user provides.

### What is Process Contention Scope?
The thread library schedules user levels threads to run on available LWP

### What is System-Contention Scope?
When deciding which kernel-level thread to schedule on the CPU


> [!Info]
> Priority is usually from $0$ to $40$
> $0$: high priority
> $40$: low priority



### 5.1 What are the two bursts that CPU schedulers are designed around?
CPU burst and IO burst

### 5.2 True or False? Under preemptive scheduling, when a process switches from the running to the ready state, it may lose control of the CPU.
It may lose control over the CPU

### 5.3 List at least three different criteria for designing a CPU scheduling algorithm.
- Small waiting time
- High thrugput
- low responsetime

### 5.5 True or False? The multilevel feedback queue scheduling algorithm allows processes to migrate between different queues.
True

### 5.6 What scheduling algorithm assigns the CPU to the process that first requested it?
First- come, first-served

### 5.7 What scheduling algorithm assigns the CPU to a process for only its time slice (or time quantum?)
Round Robin

### 5.15 What are the two general types of real-time scheduling?
**Soft** real time and **Hard** real time



