**basic concept is that we dont have to use all of the pages that are connected to the process, therefore it seems like there are more memory than there is**. Demand paging is what is doing this. With page replacement incorporated the virtual memory is increased even further, because we can swap out frames effectively seeming like they are there in memory, but will cause a page fault if not there.

# background 
As a restriction of a program is, that for it to run, it needs its entire logical address space in memory. However most processes does not need their entire program to be executing properly.
fx
* Error handling (most often not needed)
* Array lists, sets ... (often allocated more memory than needed)
* Certain operations may be used very rarely.

Would like to execute program which is not entirely in the main memory
benefits:
* Program is not limited by the amount of physical memory, can have larger virtual memory
* As filling up less memory per process, more processes can be in memory => increased CPU utilization
* Less I/O as each process does not load in its entire memory. Programs run faster.

![[Pasted image 20230605092011.png]]
When holes in the middle, it is known as a **Sparse address space** allowing for more flexibility, when the heap or stack needs to grow.
![[Pasted image 20230605092235.png]]
allow using of
* Shard libraries, which are read-only but are perceived by the processes to only be owned by them
* Shared memory, a process can create a shared memory segment and the other processes can talk with it like a shared paging, but with write access as well
* Pages can be shared between child and parent (Copy-on-Write)

# Demand Paging
Only load the pages when they are used. 
* As filling up less memory per process, more processes can be in memory => increased CPU utilization, and increased multiprogramming
* Less I/O as each process does not load in its entire memory. Programs run faster.


## Basic Concepts
Need hardware support, telling whether or not the page is in memory or not. If invalid it is either not in memory or it is not belonging to the address space of the process. When valid just access it.
![[Pasted image 20230605092746.png]]

When accessing invalid page => page fault is generated, trap is issued to the operating system.
the OS does:
1. Check internal table (stored in PCB) to see if the memory access was valid or invalid 
2. If invalid, segmentation fault and terminate the process. If valid locate the page, and pull it in
3. Find free frame to store the page
4. Store information to the allocated frame
5. Modify page table such that it keeps information for the page and set the valid bit to valid
6. Restart the execution
![[Pasted image 20230605093238.png]]

**Pure-demand paging** is when a process is loaded into memory with no pages, and first when it executes its first instruction a page is put into memory, faulting and moving on to the next instruction.

**example** Instruction ADD A B into C. First fetch and decode instruction, fetch A then B, add together and store it into C. If C's page is not currently in memory, a page fault is generated, and the entire process of ADD must be made again. However page faults are rare, so the duplications happens rarely.
**extreme case** is fx when moving a 256 byte stream from one location another, where the moving is on overlapping pages, possibly page faulting in the middle of the transfer. More trouble happens if the source block has been modified during the moving. 
1. solution may be to check if the blocks which are moved to are in memory, and only then moving
2. solution may be to have temporary registers to hold the value, and if page fault occurs, rewrite the data.

This case serves to show that demand-paging may be hard to implement

## Free-Frame List
A list of all the free frames is kept intact by the OS
The frames are usually 0'd out before, since security risks may arise if not
At startup the entire memory are in the free-frame list. When the list empty, it must be repopulated

## Performance of Demand Paging
Can be calculated by
$probability\ of\ page\ in\ memory \cdot access\ time\ +  (1- probability\ of\ page\ in\ memory) \cdot page\ fault\ time$

The effective access time is directly proportional to the `page-fault rate`, since it is so fast to access a page in memory, and to retrieve the page from disc or 2nd'ary storage takes so long.
1. One solution is to copy the entire process' pages into a swap space since it is faster than going to disc, but this will also eliminate the logic of demand paging.
2. another solution is to demand-page the first page, and then when replace, write them to swap space. This will still ensure that pages which arent used at all are not put into memory or secondary storage

### Mobile systems
Uses demand paging but the disc is the backing store. Reclaim read-only pages if needing memory.

# Copy-on-Write
Page-share with a parent. Rapid startup of processes, limiting page faults
The shared pages are marked as `copy-on-write`. If either of the processes write, a copy of the shared page is created.
![[Pasted image 20230605101632.png]]
![[Pasted image 20230605101639.png]]
There exists an alternative version of `fork()` called `vfork()`, **virtual fork**, where when called the parent will wait for the child to terminate. I uses the same pages as the parent, and any modifications to these will be visible to the parent. Because no copying takes place, `vfork()` is extremely fast.

# Page Replacement
When we can have 10 frames in memory, but two processes which ultimately will max need 10 frames in memory, but only uses 5, we can have two processes in memory. This will increase the **degree of multiprogramming**, while overcommiting memory. 
>[!WARNING]
>I/O buffers can also be stored in memory, competing with the resources for regular processes in some systems. Others hold the information in a fixed storage for I/O buffers. This is **discussed in section 14.6**

If no free frames are available, the free-list is empty. What to do then?
1. terminate processes. Not the best choise, as the users should be not beware that the operating system is using demand-paging, and therefore the process should not just be eliminated. **But it still is a choice**
2. Swap out the pages to the swap space. However **most  modern operating system versions does not use swap spaces**
3. use `Page replacement`
![[Pasted image 20230605102832.png]]


## Basic Page Replacement
Find a page that is not currently being used, and free it. Done by writing its content to swap space, updating the effected page-tables that it is no longer in memory. Based on this, modify the page fault routine.
1. Find the location of the desired page on secondary storage.
2. Find a free frame:
		a. If there is a free frame, use it.
		b. If there is no free frame, use a page-replacement algorithm to select
		a victim frame.
		c. Write the victim frame to secondary storage (if necessary); change
		the page and frame tables accordingly.
3. Read the desired page into the newly freed frame; change the page and
frame tables.
4. Continue the process from where the page fault occurred.

This required two page transfers (doubles page-fault service time). One **page out** and one **page in**
>[!INFO]
>to deal with this problem, use a dirty bit. Whether it is modified or not. Read-only pages and pages which has not been modified does not need to be paged out, i.e they do not have to written to any secondary storage. This will cut the I/O time in half if the page has not been modified

We have two problems that needs to be looked into when performing page replacement.
1. **frame-allocation algorithm**: how many frames are to be allocated to each process (may vary from process to process)
2. **page-replacement algorithm**: when needing to replace a frame, which frames to replace.
Improvements to these cause major performance gains, since I/O is so expensive.

Want the page-replacement algorithm with lowest **page-fault rate**.
+ reference string
	+ made with random number generator
	+ monitor real system
![[Pasted image 20230605104949.png]]
as 01xx is already in memory, it does not generate a page fault.

Increasing the amount of memory, giving more page frames available, will decrease the number of page faults.
Want an algorithm which holds the ones that are frequently used.

## FIFO Page Replacement
Suffers from **Belady's anomaly** where increase in page frames may not decrease the number of page faults using the algorithm.

Can implement it with a queue, removing the head when the size is met.
Does not guarantee anything regards to holding the pages which are heavily used.
![[Pasted image 20230605105405.png]]
![[Pasted image 20230605105424.png]]

## Optimal Page Replacement
does not suffer from **Belady's anomaly**
simply: **Replace the page that will not be used for the longest period of time.**
![[Pasted image 20230605105718.png]]
Cannot implement without future knowledge, making it unusable as a page-replacement algorithm. 
Used to benchmark compared to other page-replacement algorithms.

## LRU Page Replacement
does not suffer from **Belady's algorithm**

Replaces the page which has not been used for the longest amount of time.
![[Pasted image 20230605110107.png]]
Most widely used, and are better than FIFO. 
Requires much hardware support.
Implement with
1. Counters. when referenced, put the time into this extra field, keeping track of when it was last used. Must keep track of this, and each replacement must search through all of its page frames in memory
2. Stack. When a page is referenced it is removed from the stack and put on top. The page frame at the bottom is the one to be replaced. Implemented with a **doubly linked list** with a head pointer and a tail pointer. With 3 elements, at most 6 pointers must be updated


## LRU-Approximation Page Replacement
Some systems does not provide hardware support. But the cheapest way to implement it is a **reference bit** which is set whenever the page frame is referenced. In the start all are 0, but when referenced they are changed by the hardware. This is not per-say LRU, but it is to some degree.

![[Pasted image 20230605112555.png]]

### Additional-Reference-Bits Algorithm / Second chance page-replacement algorithm
using a byte to as reference-bits the operating system can every interval shift all of the bits to the higher order. This keeps information about the last 8 time periods.
One that is used throughout all pages has `1111 1111` , one that was used once has fx `0010 0000`. The lowest number can be replaced.

### Second-Chance Algorithm
Like FIFO replacement. 
Check all pages, the one with reference bit set to 1 gets a second chance, and updating its time of check to when the check is done. Then reset it to 0.
implement with the **Clock algorithm** having a circular queue of all pages, going through until it finds a victim with 0 as its reference bit, updating all used once to 0 along th eway.

### Enhanced Second-Chance Algorithm
look at the **modify bit** and **reference bit** This gives preference to which to swap.
then have four cases
1. (0, 0) neither recently used nor modified —best page to replace
2. (0, 1) not recently used but modified —not quite as good, because the page
will need to be written out before replacement
3. (1, 0) recently used but clean—probably will be used again soon
4. (1, 1) recently used and modified—probably will be used again soon, and
the page will be need to be written out to secondary storage before it can
be replaced

## Counting-Based Page Replacement
* Having a counter of how many times this has been used, BUT what the ones used at the start will have a big count, but shoud be replaced.
* most frequently used. Page with smallest count has probably just been put i

## Page-Buffering Algorithms
Hold a **modified page frame list** periodicly choose random frame to be written to swap-space, ensuring  that free frames are more likely to be ready.
Can also remember which page was belonging to a specific frame in the free-list, allowing for instant mapping using no I/O. Check the pool each time.

## Applications and Page Replacement
Some applications keep track of their own memory, fx databases, OS should buffer then. Decrease in performance. 
Os lets an application access its own storage via raw I/O where the OS is not involved.

# Allocation of Frames
How many frames are to be allocated to each process which asks. 
Could always have 3 free-frames, and when a frame is allocated a page-replacement algorithm could run to allocate a new free-frame.

## Minimum Number of Frames
Must have atleast the number of frames defined by what the instruction which uses the most amount of frames will use.
The maximum is determined by the amount of physical memory.

## Allocation Algorithms
**equal allocation** all processes gets equal amount of frames. However some processes are bigger than others.
**Proportional allocation** according to the processes size, give the frames based on some formula. fx
> [!Example]
> one of 10 pages and one of 127 pages, by allocating 4 frames and 57 frames, respectively, since

### Priority
Want to give a process with higher priority more frames, put this into the formula

## Global versus Local Allocation
global: process can take from a pool of free frames, i.e it can take from other processes
local: only from its own set of frames

fx a higher priority process could use global and steal from lower priority. While lower priority can only use its own or steal from the lower.

Global is more used, as it uses all the pages to the best utilization

## Reaper
Define a threshold for how many free-frames there must available. When it drops below this number, a kernel routine, **reaper**, is started reclaiming pages according to some page-replacement scheme.

Can be more aggressive, fx it can use FIFO for maximum speed, or less aggressive use LRU.
>[!INFO]
> in extreme cases the out-of-memory killer is spawned, killing the process which has the highest OOM-score. This is corresponding the amount of memory it is using


## Non-Uniform Memory Access
Managing which page-frames are stored in which CPU's local memory can significantly affect performance in NUMA systems.
A NUMA aware virtual memory system will allocate the page frame as close to the CPU as possible.
Keep track of which CPU the process runs on, and scheduale put the page-frames into that memory.
For threads, Linux solves this by the CFS scheduler, only allowing threads on the same NUMA node/domain.

# Thrashing
If not enough page-frames are free to store the page-frames in the working set, as process must swap one in its working set out with another process which is also in its working set, which be swapped out again soon to swap back. 
If a process spends more time paging (page replacing) then it is **thrashing**.

## Cause of Thrashing
If the CPU utilization is low, the system will increase the degree of multiprogramming (put more processes into memory), swapping out the working sets of the previous processes with the new ones. This increases page faults in the system, which means they will try access the paging-device, putting themselves in the wait queue. This will again drop the CPU utilization which will try to put more processes in memory swapping the processes working sets out again. **The throughput plunges**

**The problem is all the page faults which are generated, putting all in the wait queue for the paging device**.
![[Pasted image 20230605123144.png]]
Mostly an issue for global page-allocation and page replacement. However i local, 1 process can still thrash, putting itself in queue for the paging-device many times and for a long time, increasing the amount of time you must spend in queue for the device.

We need a way to know how many page frames the process needs, and also which are in the working set.
As a process is executing its working set changes. Fx going from `main()` to a function call
![[Pasted image 20230605123520.png]]

How many frames should be in memory for the process, is the size of its working set in the **locality**. When changing locality, a page fault is generated almost for each page-frame and then first when its locality changes again. If the process is not accommodated this size, it will thrash due to all the page faults it generates.

## Working-Set Model
Based on the locality, the working set is the unique pages used in the last fx 10 pages touched.
If the sum of working-set lengths is bigger than the amount of page frames available, thrashing will occur.  Therefore it should be less. If it is less, then another process can be put into memory. If the sum of working set lengths increases above the total page frames, a process must be suspended.

## Page-Fault Frequency
Based on the page-fault rate of a process, we can keep track of this for each process. If a processes page-fault rate falls too low, we may take some pages away from it. If it is too high we should allocate more frames to it. **If the page fault rate increases and no free frames are available => swap out a process or suspend it**

## Current Practice
Have enough memory to keep all working sets in memory.


# Memory Compression
Instead of swapping out the entire working set, compress multiple together, which later can be decompressed. As mobile systems does not support swapping, compressions is mostly used. Algorithms for compression and decompression takes both computational efficiency into account and the compression rate.

# Allocating Kernel Memory (read the summary for this part)
Free list is managed by the kernel

## Buddy System
Have one block of free memory which is divided into powers of 2 until at block is low enough space to just fit the process
![[Pasted image 20230605131020.png]]

## Slab allocation 
A cache for each of the kernel data structures. Fx. processes, semaphores...
![[Pasted image 20230605131115.png]]
initially all slabs (collection of blocks in varying sizes) are free. When used they are marked as used.

Let’s consider a scenario in which the kernel requests memory from the
slab allocator for an object representing a process descriptor. In Linux sys-
tems, a process descriptor is of the type struct task struct , which requires
approximately 1.7 KB of memory. When the Linux kernel creates a new task,
it requests the necessary memory for the struct task struct object from its
cache. The cache will fulfill the request using a struct task struct object
that has already been allocated in a slab and is marked as free.
In Linux, a slab may be in one of three possible states:
1. Full. All objects in the slab are marked as used.
2. Empty. All objects in the slab are marked as free.
3. Partial. The slab consists of both used and free objects.

The allocator will first try to request an object from a partial slab. If no one are there, start in a new slab.

Benefits:
1. No memory is wasted due to fragmentation. Fragmentation is not an
issue because each unique kernel data structure has an associated cache,
and each cache is made up of one or more slabs that are divided into
chunks the size of the objects being represented. Thus, when the kernel
requests memory for an object, the slab allocator returns the exact amount
of memory required to represent the object.
2. Memory requests can be satisfied quickly. The slab allocation scheme is
thus particularly effective for managing memory when objects are fre-
quently allocated and deallocated, as is often the case with requests from
the kernel. The act of allocating—and releasing—memory can be a time-
consuming process. However, objects are created in advance and thus can
be quickly allocated from the cache. Furthermore, when the kernel has
finished with an object and releases it, it is marked as free and returned
to its cache, thus making it immediately available for subsequent requests
from the kernel.


# Other Considerations
Major considerations for the virtual memory were page-frame allocations, and page-frame replacement.

## Prepaging
A new process in memory generates large amount of page faults at the beginning due to demand paging => store the information with the process when it is suspended.

## Page Size
Decrease page size => increase number of pages and vice versa. Think about virtual memory of `4MB (2^22)`. 
For a virtual memory of 4 MB (2 22 ),
for example, there would be 4,096 pages of 1,024 bytes but only 512 pages of
8,192 bytes. Because each active process must have its own copy of the page
table, a large page size is desirable.

Large page size is desirable.
However for internal fragmentation, we expect that the last page will only be filled up half. So here the page size is preferred to be smaller.

Transfer rate from disc is very high, but the seek time fx is very large in comparison with this. Therefore bigger page sizes are desirable.

## TLB Reach
To increase the TLB reach we must increase the page size. Fx if we increasing the page size from 2MB to 4MB we double the TLB Reach which affects the TLB hit rate greatly. But the increase will give big internal fragmentation to smaller processes. **need varying size of page sizes**. Another option is to have a **contigous bit**, if this is set, the block is contigous to another block in memory, and can be stored in one TLB-entry, such that the frame does not have to be found twice, but can just go on to the next frame next to the one stored in the TLB. fx
1. 64- KB TLB entry comprising 16 × 4 KB adjacent blocks.
2. 1- GB TLB entry comprising 32 × 32 MB adjacent blocks.
3. 2- MB TLB entry comprising either 32 × 64 KB adjacent blocks, or 128 × 16
KB adjacent blocks.

## Inverted Page Tables

## Program Structure
If the compiler or the progragmmer is aware of the paging, may lead to improvements. Fx initializing array of 0s. Row based: 1 page fault per index, i.e when the `i` changes in `[i][j]`. May lead to a page fault each iteration of a loop if going through `i` first and then `j`.
Use of stack is always to the top element => good locality
use of hash table => bad locality, as the memory may be scattered.

compiler:
* can place routine on the same page
* code that calls each other on the same page

## I/O Interlock and Page Locking
We must be sure the following sequence of events does not occur: A process
issues an I/O request and is put in a queue for that I/O device. Meanwhile, the
CPU is given to other processes. These processes cause page faults, and one of
them, using a global replacement algorithm, replaces the page containing the
memory buffer for the waiting process. The pages are paged out. Some time
later, when the I/O request advances to the head of the device queue, the I/O
occurs to the specified address. However,  this frame is now being used for a
different page belonging to another process.

1. solution: Write data to system memory and then to disc. High overhead.
2. solution: Use extra bit to tell if block is locked. When starting to copy, lock the frame. When done, unlock making it useable for others. **Low overhead.**

Kernel memory is often locked.
System calls exists for allowing processes to do so.

**Locking can also be dangerous**: If the  frame gets locked, but not unlocked due to some bug fx.

# Operating-System Examples
## Linux
manages kernel memory using SLAB allocation 

demand paging with free list of frames. Global page-replacement. uses approximately **LRU -approximation clock algorithm**

Inactive List: not recently used frames
active list in use pages frames

accessed bit

When a page is referenced it moves to the rear of active list. The LRU page is at the front of the active list. When the active list has grown to large, move the front to the inactive list.  Frames may move from inactive list to active list if they are used again
Linux has daemon **kswapd** which is woken up periodicly checking if memory is low and will free the blocks in the inactive list.

## Windows
shared libraries, demand paging, copy-on-write, paging and memory compression.

