[[ch18-2022.pdf]]

# Different types of hypervisors
[[ch18-2022.pdf#page=6]]
## 0
the VMM-software lives in the firmware.
**dont really exist** 

## 1
The VMM is seen as the system itself then.
VMMs that runs the virtual machines

## 2
runs in application

# VMs
DO NOT virtualize the kernel. Use the kernel on the hosting system

commands in the VM are just mapped to the hardware on the hosting machine, therefore it is fast

When switching to kernel-mode in the VM, we have an extra bit in the hardware for if we are in kernel mode in the hardware but in the VM

# Emulation

# Paravirtualization

# Unikernels
only takes whatever it needs to build a specific kernel, i.e slimmed down just so that i can only run and almost nothing more

using **kraft**


---
# Overview
You want to virtualize hardware and then run an OS on it.

**Host** underlying hardware system
![[Pasted image 20230614120045.png]]
Here we have hardware, could be a macbook. With an OS, could me macOS. you can run regular tasks on this, and you can use a VMM as a type 2 hypervisor. This will have virtualized hardware and an OS. Here you can boot fx a type 1 hypervisor with fx XEN which can boot other operating systems.
## Hyperviser
A hyperviser is a virtual machine manager.

### Type 0
The virtual machine manager IS the hardware
**Does not really exist**

I/O is a challenge as you have to dedicate an amount of CPUs to each guest. But you also need to dedicated the devices for each guest such that you can talk with the I/O. Circumenvention may be that you have a daemon running to handle these. 

### Type 1
You boot the virtual machine manager and you then only have virtual machines to use. The operating system is the virtual machine manager 
Fx the [[ch18-2022.pdf#page=5]] 

### Type 2
Download and application which is the virtual machine manager. The VMM is just a process running. Usually is slower than Type 1 and 2, however as there has come much more support VMs this gap has been lowered much.


# Implementation of VMMs
## Paravirtualization
Means that the VMM will provide drivers to let the VM, which it is running, know that it is a VM providing better ways of going through the hardware, i.e optimizing the hardware use for the VM

This is in contrast to what was the initial idea of the VM should not know it is not just an OS. However as hardware support increases we dont need this feature so much.

## Programming-environment virtualization
Not really virtualization, but fx specific programming languages (java) needs to run with some features which are included in the JVM.
This corresponds to providing an API which the programming language can use. Java fx runs on any operating system which supports the JVM. This is similar to interpreted languages, translating the calls to the correct CPU instructions.

## Emulation
Take each assembler instruction and translate it into another assembler instructions such that your CPU can run something with different assembler instructions.

## Application containment
**only one kernel running**
Not a virtual machine, but provide **virtual-like features** since it uses the kernel of the host. Fx docker and kubernetes. Fx running Ubuntu 12.04 on a Ubuntu 20.04 kernel you will still see when checking which kernel the Docker container is running that it runs 20.04

Docker is usually used to make small containers, fx for webservices, each with their own purpose, which can then portmap a port on the host to a port in the container. Kubernetes is then used to manage all of these running containers.

# Benefits and features
**Snapshots** of the system are usually implemented
**Live migration**: move a running VM from one type of hardware to another hardware
**Freeze** the VM
**Clone** the VM

# Problems
Hard to implement an exact duplicate of the underlying system. Meaning that you create a virtual CPU, and when the VM is context switched onto the CPU the information in the virtual CPU is put on the CPU.

## Trap and emulate
**Kernel mode** as fx type 2 hypervisers uses the kernel of the host, when switching to kernel mode, it should not switch to kernel mode on the host, but only in the VM. This means that you introduce **virtual kernel mode**. When the VM tries to make a kernel-mode instruction this generates a trap in the host, which the VMM will catch, and will analyze what has happened and execute the instruction if in virtual kernel-mode.
This means that when attempting to do **kernel-mode operations** VMs are slower, due to this extra overhead.

### Hardware support
**CPU mode**: Most hardware today has implemented hardware support for virtualization. Fx the CPU may have an extra bit for if it is in virtual kernel mode or virtual user mode. This will circumvent the situation where you have to trap and emulate, since no trap is issued, caught and analyzed.

**Nested page tables**:  accessing main memory is also virtualized. Using virtualized pages which has to be mapped into real pages.


## Binary Translation
**speacial instructions:** Some instructions behave differently when in kernel-mode than in user-mode. Since **trap and emulate** has to catch a trap, and the instruction wont issue one, the VM will just execute in user-mode not doing what it was supposed to do. Fx the **popf** in x86 architectures. 
This is dealt with by using checking the virtual kernel mode bit, and if it is in kernel mode, it will execute instructions which emulates what the real CPU would do.

However the hardware support is so high now, that it does not cause much of a performance drop

# Nested page tables
basically just a hierarchical page table, mapping from virtual memory into actual memory via virtual pages and regular memory pages 
![[Pasted image 20230614102525.png]]


# I/O
XEN, a type 1 hyperviser implemented a **circular buffer driver**, such that the guests can guests can put I/O requests into this buffer, minimizing the complication between what to do with the I/O (fx which one to handle first), just implemented a queue.

# OS overcommitment
**Important** to mention that **hardware support** exists for all of the problems.

## CPUs / VCPUs
The VM can overcommit to the number of CPU's. I.e the VMM will dedicate a CPU to the VM but the VM may have 2 VCPUs. This is overcommitment since the VM actually only has 1 CPU

VMM has to schedule the threads accordingly, using some CPU scheduling algorithms.
SInce it overcommits the CPUs the VM may not get the time units it expects. This can fx lead to **incorrect time of day showing**.

There is hardware support for this.
## Memory
Memory may also be overcommitted, which leads to the host OS having to deal with the issue. This can be fixed by some hardware support, letting the VM talk with a memory management unit directly.

## I/O usage
The less I/O requests the better, since it takes a more complicated path. However there has come hardware support providing better ways through the hardware.

# Live migration
Can just export a snapshot and run it in the VMM on a different guest.

[[ch18-2022.pdf#page=47]]
> comment to step 5. You do it until the dirty pages become so small that you just freeze the system you are migrating transferring one last time.


# Unikernels
A kernel which just can do the bare minimum, very lightweight. Can be made boot-able. Very secure since it just has the libraries it needs and **nothing more**
	The unikernel is made with a program, fx a program which can just print `hello world`, you make the unikernel bootable and you end up with a "hello-world-kernel" which can only do this, since all the other kernel properites and libraries were removed (fx system calls were removed which was not needed). 
