[[ch11.pdf]]

ctry to keep ssd on a load of less than 80% as then we need to erase more often (garbage collect), which is slow

magnetic tapes are cheap and reliable, compared to ssd and hard disks

Algorithms for how the arm should move around in the hard-disks

# Raid
fx have 8 discs, can use all at once for efficiency. Or use 1 and use rest for less data loss insurance. 
## Different levels
Raid 1,2,3 ... For how to use the additional discs for data loss  insurance. A failing desk can then just be swapped out with a working copy if fx using raid 1.

striping: using all disks for efficiency 

parity bits: xor between all the bits in the blocks, to know if some data-loss/data-flaws of bit flips, or lost memory.

11.7: `formatting`

11.10 performance(striping) and disk failures

# raid
raid 4 is not used as much since we have to touch the parity disk each time we touch another disk. This causes a bit of overhead


----------------------------------------------------------------------------------------------------------------------
# remember
+ Logical and physical formatting. 
+ A set of tracks makes up the cylinder
Raid 1+0 and Raid 0+1

# Mass storage structure
HDD and NVM. 
look at the specific measures at [[ch11.pdf#page=4]]
![[ch11.pdf#page=5]]
## Hard disk drives
read about these in the slides

### performance
move the arm is expensive: miliseconds area
rotation of the spindle: miliseconds area
**miliseconds areas are really bad**

## NVMs
fx SSD and USB-drives. Much faster. bus becomes bottlenect => connect right to PCI. No moving parts. 

It wears out. Can only write to a page ~ 100.000 times
Read and write to NAND blocks, which is a block of pages. If wanting to write to a page, you have to erase the entire block, and then write. Errassing is expensive. We cannot override a specific byte without errasing the entire block. This means that fx 20% should always be available of the memory, since it has to copy all of the information to another block before writing.  
need to write equally to all pages, since they wear out. This is done by a controller in the SSD, i.e not the OS.

If the memory is filled up above fx 90% the performance will go much down, because of needing work in the block fashion. We need a garbage collector which will ensure that we are under some threshold such that we dont need to erase that often during the writing, since the erasing takes time and should be done in the background

The erasing is done when we have modified fx a file, then we have to invalidate the previous file state, and erase it.
![[Pasted image 20230611151833.png]]
![[Pasted image 20230611113315.png]]
	
# Volatile memory mass storage
Much faster than both HDD and NVM, but also smaller. Fx having 32 or 64 GB main memory, format 2GB to make a **RAM drive**. `/dev/ram`
![[Pasted image 20230611152207.png]]
NVMs are getting cheaper compared to HDDs

# Magnetic tapes
Magnetic tapes are significantly cheaper also compared to HDDs when compared to dollar per TB. However they are very reliable, but also super slow. Still used heavily for mass storage which does not require random access nor speed.

# Disk attachment
Connected via busses, such that the computer installed controller can talk with the device driver for the storage
[[ch11.pdf#page=15]]

# HDD Scheduling 
## Disk Scheduling
Measured in the the amount of movement of the cylinders
### FCFS
![[Pasted image 20230611154118.png]]

### SSTF (Shortest seek time first)
Can lead to starvation. Usually cannot implement this because of this flaw of starvation
![[Pasted image 20230611154259.png]]


### SCAN
Start where you are, and go all the way to the left, then to right, then to left...  **The problem is then that if you start in the middle and move all the way to the side, then you probably will not have any request in the from the side to the mid again**. 
[[ch11.pdf#page=25]]

### C-SCAN
Will move to the right and then all the way to the left, and start over again moving to the right. This fixes the problem shown above
[[ch11.pdf#page=27]] 

## Selecting a DISK-Scheduling Algorithm
[[ch11.pdf#page=28]]

# NVM scheduling
Perfomance decreases with age
If memory is filled up, can decrease performance drastically

# Error Detection and Correction
bit flips
- Parity bits to check
- Cyclic redundency check
- Error-correction code


# Storage device management
- Low level formatting / physical formatting
Add sector size and block size. Fx having sectors of 4KB and 512KB blocks

- Logical formatting
Putting a file system on top of the low level formatted disc. This initializes file-system structure, fx storing a list of not allocated and allocated blocks.


**Root partition** the partion/volume on the mass storage which holds the OS.  Other partitions can have different OSes. At mount time the error detection bits are checked for inconsistency, fx if power were switched off, and in the middle of updating a block, what to do with this block? May have to just throw it away if cannot fix it, else try to fix it and start again. 

Boot block points to the boot loader which knows how to boot the kernel, or if multiple OSes, use a boot management programming for locating the multiple OS booters.

## Device Storage Management
MBR master boot record which holds information on where to find the OSes to be booted. 512 bytes in size, like the usual block size.


# Swap-space management
low-level formatted either in the same partition as the OS or in a different partition. Can either live in some formatted blocks. 
**Swap space are rarely seen as the performance goes down drastically when used**.

# Storage attachment
* fiber channel (very fast.)
* network attached storage. Via LAN or WAN, using iSCSI
	* Storage area networks. Uses array of storage. The Storage  is hot swapable
* Cloud storage. Slower than network since API based not iSCSI. 

# Raid Structure
Used to increase **mean time till data loss** when having a lot of storage. Mirroring for example almost is a quadratic addition to the provided mean time till data loss

**Striping** uses multiple discs as one storage unit. This can increase parallelism
Important once:
+ RAID 0 (pure performance, only striping)
+ RAID 1 (have to write to the copy each time you write to the actual disk which is copied, but good for mean time till data loss)
+ RAID 4 (have to write to the parity block each time you write, therefore not used in practise as the parity disk is used much more frequently)
+ RAID 5 (better than RAID 4 since the parity blocks  are spread out equally on the discs, such that one disk is not used much more frequently)
	+ RAID 4 and 5 use much less redundency of the disk usage than fx RAID 1

**This can both be done in hardware and software**
* Hardware: you tell the controller to the storage, please configure yourself to use x-RAID
* Software:
![[ch11.pdf#page=45]]

# Object Storage
Store entire objects instead of the usual files, where you can navigate via directories. Steps are that you send some data which is an object, fx a `.png` file which is stored in an amazon bucket, you then get an ID which can be used to access it, and there are no other way to access this than using this ID. This is typically implemented with copies of the object aswel, to ensure clients they are not going to lose data with some certainty.