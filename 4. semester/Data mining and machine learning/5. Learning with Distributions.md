![[_DM566-2023-part5.pdf]]
[variance computation](https://www.mathsisfun.com/data/random-variables-mean-variance.html)
![[Pasted image 20230314100354.png]]



# Part 1
## Keywords
* random variable associated with the event
	* random variable is not per say an actual variale. Its a function mapping the a sample space, or subset of a sample space, to a number.
* independent events based on random variables
* Expectation/mean
![[Pasted image 20230307103720.png]]

* variance
How much the random variable X (which again is a function, mapping a sample space to a number) differs from the expectation of X (look expecation).
[example](_DM566-2023-part5.pdf#page=25)
> notice that the expectation is multiplied on each summation.

* standard deviation
* covariance
	* how much one variable diverges from its expectation, times the other variable divergence from its expectation
* correlation
![[Pasted image 20230307105046.png]]
since we cannot tell anything, about x from y and vice versa, see the example on figures relevant to each possible coveriance
* median and mean


## Recap
* Random variable
* Expectation
* Variance


# part 2

## keywords
* Probability distrubutions (area under the graph, for which the area under the entire graph is 1)
* probability in infinite sample space
	* infinitely many options => probability of random variable X taking one specific value, is 0.
* Cumulative distrubution function => integral of the function smaller than or equal to a.
	* can also do between a and b i.e $a\leq X\leq b$. Do the integral between.
* Joint distribution
* Uniform distribution
	* The area between a and b is summing up to 1, i.e the probability distribution sums up to 1 between theese, and is 0 elsewhere
		* Can also be used in multi-dimension,fx. the volume of a square in 3d just have to sum up to 1
* Normal distribution
	* If you have multiple random variables that may behave randomly, if we accumulate the sum of theese it will follow the normal distribution
	* Standard normal distribution
	* Geometrical meaning of mean and variance
		* The lower the variance the bigger the peak. because the sum of area on the graph still has to be the same 
	* Can be used in 3d aswel, the **Bivariate Normal Distribution** 
* Standarzing via **z-score**

## recap
* Commulative distribution function (CDF)
	* $Pr(x\leq a)\in[0,1]$
	* $Pr(X\leq+inf)=1$
	* $Pr(a\leq X\leq B)=Pr(X\leq b)-Pr(X\leq a)$


# Part 3
## Keywords
* Data set (design matrix)
* Different interpitation of a single algorithm
	* algebraic: as a vector, calculating the distances
	* statistical/probabilistic
		* EM clustering
			* Cluster is represented by a probabilistic density function
				* fit multiple normal distributions over each cleaster
					* to figure out which cluster the point is part of, set a threshold for the y-value i.e draw a line on the y-axis and decide which x-value has its corresponding y-value high enough to be under some y-value-threshold, and the point must then be under the normal distribution
				* Can then figure out with a probability fx if point b belongs to cluster 1 or cluster 2 with a probabilities. I.e cluster1 = 87%, cluster2 = 20%
			* worse than k-means, when looking into the runtime. Even tho the probablistic approach is fully paralellizable, because there is one step that costs so much
		* Parametric vs Non-parametric Learning [[_DM566-2023-part5.pdf#page=116]]
## Recap
* Each cluster may be **Gausian** or **Normal-distributed** in parametric approaches to clustering
	* Can get rid of all data when we learn the mean and variance in the (the normal distribution) to see where  the most compact areas are, after we have learn the Gausian or normal-distribution over the training set.
* Non-parametric
	* have to pass all the data when learning again or assigning points, will also be biased towards the data you have.
# Part 4
## Keywords
* High density regions (if density in some region if above som threshold)
	* populated area in the data will give clusters.
		* Threshold being too high or low is not good idea.
			* high: may not get as many clusters that is optimal
			* low: two clusters may be grouped together that should not

* parameters for density-based clustering
	* hyperparameters, parameters that does not change during training
		* `minpts` also counts the point we are checking within. So if a point has 1 neighbour it meets requirement of `minpoints=2`
		* see slides
		* Density-reachable -> can reach the other point through points that has high density with respect to the threshold.
		* Density connected -> can reach other point, but via a point that has not high density
	* **DB-scan-algorithm** uses `minpts` logic
		* Distance
		* can also use **Shared-nearest-neighbours (SNN)** 
* Hierarchical clustering
	* having clusters inside clusters
		* build via trees
		* **Agglomerative Algorithm** page 142 (_pros  & cons_ on page 151)
			* Complexity = $O(n^3)$
			* Distance-methods can be choosed on which clusters to combine each step
				* see from page 145
				* Single Link (distance of the closest points in the two clusters)
				* Complete Link (maximum distance of two points in clusters)
				* Average Link (average distance as distance score of 2 clusters)
		* **OPTICS algorithm**
			* Valleys = clusters
			* parameters: epsilon (distance), minpoints will smoothen the graph
				* OPTICS Plot is called the reachability diagram
			* Can use this for detecting how many clusters there should be, and then run k-means afterwards with this information
			* Hierachical clustering in this plot can then be seen from the fact that distance between the points will be lower and lower, and the lower distances will be a better cluster inside the bigger cluster


* Outlier detection
	* whatever my model cannot capture, doesnt know what it is
	* One-class classification
		* Only know labels of normal class, and others are outliers
	* Distance-based  outliers
		* Distance to nearest neighbours
			* can then go the nearest neighbours, nearest neighbours and check again to see if it is just a small cluster or it is two neighbours being 2 outliers
			* 