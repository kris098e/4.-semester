TR=TRaining set
TE=TEst set
![[_DM566-2023-part3.pdf]]

domain D = feature space
C = {c1, c2, c3} = classes/**Labels**/**Oracle**/**Ground-truth-labels** 
$f:D\to C$

Predictive-truth-labels are when we predict from a training set something else that isnt inside the known set we trained on

from training set get the hypothosis function that should map $h(o) = f(o)$ where $h(o): D \to C$ as closely as possible.

good to have a function outcome that we know what happens, i.e not just two magical numbers multiplied onto some value. Easier to sell in the real world, as buyers wants to know what they are buying


Agorithms can have **INDUCTIVE BIAS** (_restrictions on the classification function_), fx. having strict lines seperating the class or curly lines or perfect cirkular lines
Algorithm of choise for machine learning comes with **certain biases** Herefore we can choose which bias we want when we have some domain knowledge.

we cant know if we have the perfect apporimation $h: D\to C$ to the true function $f: D\to C$ 

Computational learning theory(COLT)????? => Vlad Vapnik??????????????????

**overfitting is bad**

more specific functions with (i.e more variables, more AND and OR operators used 
=complexity), are not better to understand what is going on. Going more general means less variables needed, singleing in on the most important variables

complexity = higher if we have more AND, OR statements.

when having too high complexity (more variables and booleans) then we are likely to overfit, and we cannot use the function on outside dataset, as much as a more general one with less variable (as this would simply be more an **rembering exercise**). There is a sweet spot to how complex it should be. Too high complexity is also called _remembering data_

**there are both underfitting and overfitting. But there is a sweet spot**

**Biace-variance dilemma**
- variance: how flexible the algorithm is

an algorithm also contain **training error** which can be calculated. We can also have **Test error**

**when training:** can use 50% of a dataset as TR and the remaining 50% is used for the TE to evaluate the output function. We can do this a bit of times and take the average

to figure out if the function **overfits** we see how similar the TR-error and compare it to the TE-error. THey should be similar. If the TR-error is very low and the TE-error is high, then we know we are overfitting

Therefore we look at the **m-fold Cross-Validation** to validate the TR-error and TE-error


can have multiple hypothosis classes/functions, an decide which one does better. i.e $h=\{h_{1}, h_{2}, h_{3}\dots\}$

**training capacity=amount of memory we train on** this is also always contains more variables when the capacity increases.?????

the difference between the test error and training error, we can just take the difference to see if they vary a lot. better if they dont vary from eachother

Nowadays the data sets and models are so big that we just choose a test-set and a training set, freeze it here, train and then evaluate. **The data is big enough, such that we only get a small gain in knowledge if we use other evaluation**

TP= true positive
FP= false positive
FN= false negative

precision=$\frac{TP}{TP+FP}$: ratio that the model for the model to make a positive prediction, how accurrately did it make this prediction
recall=$\frac{TP}{TP+FN}$: ratio of how many times we predict correcly, when the case should be predicted to positive.

# KNN (k-nearest neighbours) -> non-parametric model
1. lazy learner -> no training phase
2. Big memory usage -> if we have 1 trillion data points, we have to store the entire data set on the fx. mobile phone that runs the app.
	1. Need to query all other data points to find the closest point -> very slow
		1. i.e prediction time is proportional to the data set size

# Bias and its relationship to KNN
bias means: take into account a lot of data -> data plays less role in the decision e.g 1NN is less biased than 20NN because new data does not play as huge role for 1NN.

Bias and variance: 
* Bias -> data doesnt matter as much 
* Variance -> data plays higher role, new data might change the entire output







