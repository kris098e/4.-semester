TR=TRaining set
TE=TEst set
![[_DM566-2023-part3.pdf]]

domain D = feature space
C = {c1, c2, c3} = classes/**Labels**/**Oracle**/**Ground-truth-labels** 
$f:D\to C$

Predictive-truth-labels are when we predict from a training set something else that isnt inside the known set we trained on

from training set get the hypothosis function that should map $f(o) = h(o)$ where $h(o): D \to C$ as closely as possible.

good to have a function outcome that we know what happens, i.e not just two magical numbers multiplied onto some value. Easier to sell in the real world, as buyers wants to know what they are buying


Agorithms can have **INDUCTIVE BIAS** (_restrictions on the classification function_), fx. having strict lines seperating the class or curly lines or perfect cirkular lines
Algorithm of choise for machine learning comes with **certain biases** Herefore we can choose which bias we want when we have some domain knowledge.

we cant know if we have the perfect apporimation $h: D\to C$ to the true function $f: D\to C$ 

Computational learning theory(COLT)????? => Vlad Vapnik??????????????????

**overfitting is bad**

more specific functions with (i.e more variables, more AND and OR operators used 
=complexity), are better to understand what is going on. Going more general means less variables needed

complexity = higher if we have more AND, OR statements.

when having too high complexity (more variables and booleans) then we are likely to overfit, and we cannot use the function on outside dataset, as much as a more general one with less variable (as this would simply be more an **rembering exercise**). There is a sweet spot to how complex it should be. Too high complexity is also called _remembering data_

**there are both underfitting and overfitting. But there is a sweet spot**

**Biace-variance dilemma**
- variance: how flexible the algorithm is

an algorithm also contains **training error** which can be calculated. We can also have **Test error**

**when training:** can use 50% of a dataset as TR and the remaining 50% is used for the TE to evaluate the output function. We can do this a bit of times and take the average

to figure out if the function **overfits** we see how similar the TR-error and compare it to the TE-error. THey should be similar. If the TR-error is very low and the TE-error is high, then we know we are overfitting

Therefore we look at the **m-fold Cross-Validation** to validate the TR-error and TE-error






