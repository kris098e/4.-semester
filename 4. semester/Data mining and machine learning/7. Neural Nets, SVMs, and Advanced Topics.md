![[_DM566-2023-part7.pdf]]

# How to train neural network
use $$backprop = chain \ rule + gradient \ decent$$
gradient decent uses:
$$x=argmin(f(x))$$
We start from a point $x_{0}$ takes its derivative and figure out how to update to find the next x to go to by taking $x_{1}=x_{0}-f'(x_{0})$. 
learning rate is how fast we decent, i.e using $\alpha$ being very high in
$$x_{t}=x_{t-1}-\alpha \cdot \nabla  f(x_{t-1})$$
using $\alpha=10^{-3}$ is frequently the optimal weight

By choosing  $\alpha=10^{-3}$ the calculation for the next $x$ remember the next $\frac{1}{\alpha}$ steps. I.e using  $\alpha=10^{-3}$ we remember the previous 1000 steps
