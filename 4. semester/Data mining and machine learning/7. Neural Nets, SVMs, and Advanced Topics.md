![[_DM566-2023-part7.pdf]]

# How to train neural network
use $$backprop = chain \ rule + gradient \ decent$$
gradient decent uses:
$$x=argmin(f(x))$$
We start from a point $x_{0}$ takes its derivative and figure out how to update to find the next x to go to by taking $x_{1}=x_{0}-f'(x_{0})$. 
learning rate is how fast we decent, i.e using $\alpha$ being very high in
$$x_{t}=x_{t-1}-\alpha \cdot \nabla  f(x_{t-1})$$
using $\alpha=10^{-3}$ is frequently the optimal weight

By choosing  $\alpha=10^{-3}$ the calculation for the next $x$ remember the next $\frac{1}{\alpha}$ steps. I.e using  $\alpha=10^{-3}$ we remember the previous 1000 steps

Gradient decent is the way the neural network learns

For each layer, we take the input from the last layer and then use it for the activation function. Lastly we will come to an output.

# how is this learning
it is learning the given weights for each activation functions 

# Architecture of a neural net
## Which activation function for each layer
an algorithm/neural network can do this for us the best.

# Chain rule for vector-variate functions
Start from the layer closest to the output, and back-propagate down to find the loss of the neural-net. This can be feeded into an optimizor which will update the weights to something better.
If the error is large, the weights are updated drasticly since we make a lot of error. If it is lower it will update more carefully, which means it updates less each feed.

## The backprob algorithm


# Support Vector Machines (SVMs)
Understand the practical part of **SVMs**

* nonlinearity: feature extraction 
* linearity: decision boundary

Extracting some data into another feature space. The data may be **non-linear**, but we extract it into another feature space, which will be able to seperate the classes with **linearly**.
[[_DM566-2023-part7.pdf#page=40]]

## Kernel
measures similarity from two points as an input. a score of 1 means it is the same point

## which seperating hyperplane to chose
should choose the one to the right. Introduce less bias, as if we choose the one to the left, we make bias that the seperation should be closer to the classes, which we have to actually explain.
It has **less margin**
[[_DM566-2023-part7.pdf#page=52]]

### margin
The margin is the distance from a data point to the hyper-plane.

#### Optimization
optimization of the margin, means maximimizing the distance from the closest point from the classes to the margin.

### Support vectors
**Are the one that is closer to the decision margin / the hyper plane**

It is enough to learn only the support vectors, and we can forget all the other data points.

### parametric or non-parametric model
If the model is parametric, we can throw away all the data after we have learned some parameters. 

#### The kernel function is a non-parametric model


### When  the data is not perfectly linearly seperable
introduce a **soft margin**.
Introduce variables **ξ n ≥ 0** that allow violation of the margin
but with a penalty linear to the distance from the margin.
[[_DM566-2023-part7.pdf#page=64]]. "in the margin" means it is inside of the maximized margin, i.e closer to the decision boundry than the distance from the margin to the decision boundry

## Important
### Support vectors
[[_DM566-2023-part7.pdf#page=61]], the highlighted points are support vectors, even tho the data is not linearly seperable
**Are the one that is closer to the decision margin / the hyper plane**
It is enough to learn only the support vectors, and we can forget all the other data points, as this will be  what in the end was used to maximize margin.

**consequence of margin maximization**
### Kernel function
The kernel function is what is applied on each data instance to **map the original non-linear observations into a higher-dimensional space in which they become linearly separable**.

**increase capacity**
### Margin maximization
[[_DM566-2023-part7.pdf#page=53]]
**increase robustness**
Margin maximization is a key concept in Support Vector Machines (SVMs), which are a type of supervised learning algorithm used for classification and regression analysis.

In SVMs, the margin refers to the distance between the hyperplane (decision boundary) that separates the classes and the closest data points from each class. The goal of margin maximization is to find the hyperplane that maximizes this distance or margin, while still correctly classifying the data points.

The reason for maximizing the margin is that it leads to better generalization performance of the SVM model. A larger margin means that the hyperplane is less likely to overfit to the training data and is more likely to perform well on new, unseen data.

To mathematically formulate margin maximization, SVMs solve an optimization problem that involves finding the hyperplane that maximizes the margin subject to certain constraints. These constraints ensure that all data points are classified correctly and that the margin is calculated based on the closest data points from each class.

Overall, margin maximization is a key concept in SVMs that helps to ensure good generalization performance of the model by finding the hyperplane that maximizes the distance between the classes.


