![[_DM566-2023-part4.pdf]]

# part 1
_Bayesian Learning in a nutshell (similar to how humans process)_: 
prior belief(prejudice) + experience => updated belief (posterior belief)

probability of event (frequentist definition, **Objective definition**)
$$pr(event)\lim_{ \#trials \to \infty } \frac{event}{trials} $$
bayesian learning is more subjective:

## Keywords
* Sample space
	* Events (powerset gives all possible events)
* propability function
	* Sure event: pr($h$)=1, where h is the entire sample space
* propability space
* Event combinations (Overlapping, nonoverlapping, union, AND, exclusive ...)
* Combined  probability (combining event combinations and calculation propability)
	* Union bound (lemma)
* Independant events (are the events correlated?, can be calculated)
* conditional probability (probability of both things happening if some other event has happened. How much they are correlated?)
* Quality measures for association rules
	* Lift
	* Jaccard (large jaccard better for association rules)
	* conviction
![[Pasted image 20230228104854.png]]
def 1.6: probability of B given event E, weighted by the probability of E

$Pr(A|B) = \frac{pr(A\cap B)}{pr(B)} \implies pr(A\cap B) = pr(A|B) \cdot pr(B)$

## Recap
* Joint event (2 events happening at the same time $A\cap B$)
* conditional probability (some events has happened already, this has effect on the next event happening. I.e we tell you its gonna be 20 degrees tomorrw => will you take on a coat?. Conditioning on some event (the weather), for an event (taking on a coat))
	* shrinks sample space to the joint event of the conditions. i.e
		* $\frac{Pr(A\cap B)}{Pr(B)}$
* independence (Pr(A) = Pr(A|B), if B has happened it doesnt matter to event A of happening, they are independant from eachother
	* $Pr(A\cap B) = Pr(A) \cdot Pr(B)$

# part 2
## keywords
* IMPORTANT **The law of total probability for an event** (the probability given another event) $Pr(A|B) = \frac{pr(A\cap B)}{pr(B)} \implies pr(A\cap B) = pr(A|B) \cdot pr(B)$
	* bases a new event on the existing events, such taht we can calcuate the probability of this event
* Bayes' rule (how to update our believes due to causes of other things)
	* look examples
* Bayesian reasoning (analyze which model is best to explain something, since all models will work)
* bayesian approach
	* likelihood
	* evidence

* Probablistic classification
* Bayes optimal classification is the best classifier one can pick up (can be proved)
* 

## recap
* Bayesian model selection
	* $$pr(A\cap B)=pr(A|B) \cdot pr(B)=pr(B|A)\cdot pr(A)$$
	* Bayes rule$$pr(B|A)=\frac{pr(A|B)\cdot Pr(B)}{pr(A)}$$
		* where the B in $pr(A|B)$ is the hypothesis/cause
		* the A in $pr(A|B)$ is the observation/experience
		* $pr(B)$ is the prior belief in the hypothesis
		* $pr(A|B)$ is the likelihood
		* $pr(A)$ is the evidence
		* $pr(B|A)$ is the posterior belief in hypothises
* Bayes optimal classification
	* given some new input, give the new input to an existing class, calculated by propability 
* Naive bayes classifier