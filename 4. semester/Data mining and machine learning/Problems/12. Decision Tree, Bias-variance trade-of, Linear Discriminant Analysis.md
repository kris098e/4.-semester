![[_exercise12.pdf]]
#exercise_decision_tree 
# 1 Decision tree
## Create decision tree based on gini index
Find the root by calculating gini index for all the attributes (The attibute with the lowest gini index is the one which should be root).
When we have found the root, we calculate again the gini index for which is left. I.e if we choose `age` to be the root, it will have 3 branches. Then for fx >= 30, we only consider the cases where this true, and calculate the gini index for each attribute. This branch will then chose its lowest gini index. Then look at the new branches.
Next we come back to `age=31...40` and choose the attribute which will give the lowest gini index for this and go on with a new branch...

Use the formula on
[[_DM566-2023-part6.pdf#page=63]]
## THE SOLUTIONS WIRTES $\frac{1}{5}^2$ instead of $\left( \frac{1}{5} \right)^2$
### age
$\leq 30$, 3 no, 2 yes
$31\dots 40$, 0 no, 4 yes
$>40$ 2 no, 3 yes

(look at examples here)[[_DM566-2023-part6.pdf#page=64]]
$G(\leq 30)=1- \left( \frac{3^2}{5^2} +\frac{2^2}{5^2} \right)=\frac{12}{25}$
$G(31\dots 40)=1-\left(  \frac{0^2}{4^2}+\frac{4^2}{4^2} \right)=0$
$G(>40)=1-\left(  \frac{2^2}{5^2} +\frac{3^2}{5} \right)=\frac{12}{25}$

$G(split\ on\ age)=\frac{5}{5+4+5} \cdot \frac{12}{25}+\frac{4}{5+4+5} \cdot 0 + \frac{5}{5+4+5} \cdot \frac{12}{25}=\frac{12}{35}$
![[Pasted image 20230428085901.png]]
![[Pasted image 20230428085915.png]]
![[Pasted image 20230428085950.png]]
![[Pasted image 20230428090012.png]]
![[Pasted image 20230428090407.png]]
### IMPORTANT
for student no, where we only consider the cases where the age is <=30, we always have buy = no, and student yes we always have the case of buy = yes.
#### given that <= age,  and picking student as branch, we have maximum purity so we dont make further branches 

The same goes for the branch of `age= 31...40` has maximum purity aswel. THis means that gini index for this branch is 0. So it is always `buy = yes` given that `age=31...40`.

### final tree
![[Pasted image 20230428091013.png]]
## Tests
look at the decision tree, an follow down the path 
i. go down branch, look and see that student = yes, and therefore it buys, label is yes
ii. (typo, no=fair). label is yes
iii. label is yes

# Bias-variance trade-of
![[_exercise12.pdf#page=2]]
#variance Variance **refers to the changes in the model when using different portions of the training data set**. Simply stated, variance is the variability in the model prediction—how much the ML function can adjust depending on the given data set. Variance comes from highly complex models with a large number of features.
When given a new training data, how can the model be stable. How should it learn to not fluctuate given the new data 
If the variance is high, it overfits
This implicates a very complicated model

#bias is the difference between the expected prediction and the true value. The model cannot capture the complexity of the data
$bias=E(prediction) - true \ value$
If the bias is high, we most of the time underfit

#error_of_model 
The irreducible error means that the error cannot be lowered after a specific error threshold. It wont come down after som specific threshold

#kNN_properties

a. we only look at height and circumference. This is the bias that we dont care about the other attributes. The model is very simple. This may cause underfitting
b. The fact that we have some variables which can capture some complexity. But it is still low variance, since we the model does not really capture that much complexity.  
c. The attributes may not be enough. We may need other attributes to reduce errors.
d. The model has  high bias and low variance. Consider more variables being used (increase variance a bit) and lower the bias by doing this. This may result in higher accuracy 
e.  When using linear reggression model it introduces that we may have relation between the input variable and some output. We assume a linear relation between the data and the target variables. But sometimes we cannot do this linearly. 
kNN does not assume any previous relation. With small k in kNN it means that we have high sensitivety to noise and other data. This means we have high variance and low bias. If we use larger k, it becomes more biased and lowered variance, since it is less sensitive and the new data does not speak as loud for the model.

![[_exercise12.pdf#page=3]]
#lda_linear_discriminant_analysisIs formular to tell which group the data belongs to. Its a classifying technique
Given some variable to the function it provides the label/group for the variable

## Linear Discriminant Analysis
![[Pasted image 20230428095346.png]]
![[Pasted image 20230428095827.png]]
We chose class 1, as it uses ARGMAX, which in this context means that class 1 will have the higher score.
