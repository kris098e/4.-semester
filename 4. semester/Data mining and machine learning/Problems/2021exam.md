# Question 1, apriori
Given the items $l=\{A, B, C, D, E, F, C, H, I\}$
and the set of transactions $T$ :

| TransID | Items |
| --- | --- |
| 1 | A B CE G H I |
| 2 | A B D E F H I |
| 3 | A B D E H |
| 4 | A B E F H |
| 5 | A B E H |
| 6 | A D F G I |
| 7 | A F I |
| 8 | B C D E G I |
| 9 | C G I |
| 10 | D E F G H I |
| 11 | D G I |
| 12 | F |
For the minimum support of 3 , we already determined the frequent 3-itemsets with the APRIORI algorithm:
$$
L_3=\{A B E, A B H, A E H, A F I, B D E, B E H, B E I, C G I, D E H, D E I, D F I, D G I, E F H, E G l, E H I\}
$$
Which of the following 4-itemsets are preliminary candidates in the next step of APRIORI (i.e., after the merging step but before pruning)?
- [ ] ABDE
- [ ] ABEH
- [ ] BEHI
- [ ] CDGI
- [ ] CEGI
- [ ] DEFI
- [ ] DEGI
- [ ] DEHI
- [ ] EFHI
- [ ] EGHi

### Solution
- [ ] ABDE
- [x] ABEH ✅ 2023-03-24
- [x] BEHI ✅ 2023-03-24
- [ ] CDGI
- [ ] CEGI
- [ ] DEFI
- [ ] DEGI
- [x] DEHI ✅ 2023-03-24
- [ ] EFHI
- [ ] EGHi
``` Python
def find_candidates(v):
    """
    Find all candidates for the next level of the tree for the Apriori algorithm.
    """
    candidates = []
    length = len(v[0])-1
    for i in range(len(v)):
        for j in range(i + 1, len(v)):
            if v[i][0:length] == v[j][0:length]:
                candidates.append(v[i] + v[j][2])
    return candidates


l = ['ABE', 'ABH', 'AEH', 'AFI', 'BDE', 'BEH', 'BET', 'CGI', 'DEH', 'DEI', 'DFI', 'DOI', 'EFH', 'ECI', 'EHI']
candidates = find_candidates(l)
print(candidates)
```

# Question 2, confidence 
For some transaction database we found that the rule $\{A, B, C, D\} \rightarrow\{E, F, G\}$ has a confidence below the confidence threshold.
Which of the following rules will therefore have a confidence below the confidence threshold as well?
- [ ] $\{A\} \rightarrow \{B,C,D,E,F,G\}$
- [ ] $\{A,B,C,D\} \rightarrow \{E,F\}$
- [ ] $\{A,C\} \rightarrow \{B,E,F\}$
- [ ] $\{A,C\} \rightarrow \{B,D,E,F,G\}$
- [ ] $\{A,D\} \rightarrow \{B,E,G\}$
- [ ] $\{A,D\} \rightarrow \{B,E,F,G\}$
- [ ] $\{A,D\} \rightarrow \{B,C,E,F,G\}$
- [ ] $\{B,E,F\} \rightarrow \{A,C,D\}$
- [ ] $\{C\} \rightarrow \{A,B,D,E,F,G\}$
- [ ] $\{C,D\} \rightarrow \{A,B,E,F,G\}$

### Solution
- [x] $\{A\} \rightarrow \{B,C,D,E,F,G\}$
- [ ] $\{A,B,C,D\} \rightarrow \{E,F\}$
- [ ] $\{A,C\} \rightarrow \{B,E,F\}$
- [x] $\{A,C\} \rightarrow \{B,D,E,F,G\}$
- [ ] $\{A,D\} \rightarrow \{B,E,G\}$
- [ ] $\{A,D\} \rightarrow \{B,E,F,G\}$
- [x] $\{A,D\} \rightarrow \{B,C,E,F,G\}$
- [ ] $\{B,E,F\} \rightarrow \{A,C,D\}$
- [x] $\{C\} \rightarrow \{A,B,D,E,F,G\}$
- [x] $\{C,D\} \rightarrow \{A,B,E,F,G\}$

If the left side of the arrow represents a subset of $\{A,B,C,D\}$, and the right side includes $\{E,F,G\}$, then the confidence of the rule is below the confidence threshold.
And its include all. 

Left side: Subset of $\{A,B,C,D\}$
Right side: Includes $\{E,F,G\}$
Right and left: $\{A, B, C, D, E, F, G\}$

### BETTER: 
Right and left: $\{A, B, C, D, E, F, G\}$
And $|X \cup Y|=|transactions|$

# Question 3 TD2
We have the following one-dimensional dataset:

| ID  | Value |
| --- | ----- |
| A   | 1     |
| B   | 3     |
| C   | 5     |
| D   | 7     |
| E   | 10    |
| F   | 11    |
| G   | 12    |

In three attempts, k-means delivered the following three clustering solutions:
$$
\begin{aligned}
& S_1=\{A, B, C\},\{D, E, F, G\} \\
& S_2=\{A, B\},\{C, D\},\{E, F, G\} \\
& S_3=\{A, B, C, D\},\{E, F, G\}
\end{aligned}
$$
We want to compare the solutions using $T D^2$. Which of the following statements are correct?
- [ ] $S_1$ is better then $S_2$ in terms of $TD^2$
- [ ] $S_2$ is better then $S_3$ in terms of $TD^2$
- [ ] $S_1$ and $S_3$ are equally good in terms of $TD^2$
- [ ] $S_3$ is better then $S_1$ in terms of $TD^2$

### Solutions
- [ ] $S_1$ is better then $S_2$ in terms of $TD^2$
- [x] $S_2$ is better then $S_3$ in terms of $TD^2$
- [x] $S_1$ and $S_3$ are equally good in terms of $TD^2$
- [ ] $S_3$ is better then $S_1$ in terms of $TD^2$

#### $S_1$
$Centroid(\{A,B,C\}) = \frac{1+3+5}{3}=3$
$Centroid(\{D,E,F,G\}) = \frac{7+10+11+12}{40}=10$
$TD^{2}(A,B,C)=(1-3)^2+(3-3)^2+(5-3)^2=2^2+2^2=8$
$TD^{2}(D,E,F,G)=(7-10)^2+(10-10)^2+(11-10)^2+(12-10)^2=3^2+1^2+2^2=14$
$8+14=22$

#### $S_2$
$Centroid({A,B})=\frac{1+3}{2}=2$
$Centroid({C,D})=\frac{5+7}{2}=6$
$Centroid({E,F,G})=\frac{10+11+12}{33}=11$

$TD^{2}(A,B)=(1-2)^2+(3-2)^2=2$
$TD^{2}(C,D)=(5-6)^2+(7-6)^2=2$
$TD^{2}(E,F,G)=(10-11)^2+(11-11)^2+(12-11)^2=2$

$2+2+2=6$
#### $S_3$
$Centroid(A,B,C,D)=\frac{1+3+5+7}{4}=4$
$Centroid(E,F,G)=\frac{10+11+12}{3}=11$

$TD^2(A,B,C,D)=(1-4)^2+(3-4)^2+(5-4)^2+(7-4)^2$
$=3^2+1^2+1^2+3^2=20$
$TD^2(E,F,G)=(10-11)^2+(11-11)^2+(12-11)=2$

$20+2=22$

#### TD
Total Deviation Squared.
A measure used to evaluate the quality of clustering solutions in k-means clustering. It represents the sum of squared distances between each data point and its corresponding centroid. The lower the TD value, the better the clustering solution.
$$TD^2 = \sum_{i=1}^{k} \sum_{x \in C_i} ||x - \mu_i||^2$$
$k=$ Number of clusters
$\mu_i=$ centroid of the i-th cluster
$x =$ A point
$$TD^{2}(C)=\sum\limits_{p \in c}dist(p,\mu_C)^2$$
#### Python (works)
``` Python
import pandas as pd
import numpy as np

# Define the dataset
data = {
    'ID': ['A', 'B', 'C', 'D', 'E', 'F', 'G'],
    'Value': [1, 3, 5, 7, 10, 11, 12]
}

df = pd.DataFrame(data)

# Define the clustering solutions
solutions = [
    [['A', 'B', 'C'], ['D', 'E', 'F', 'G']],
    [['A', 'B'], ['C', 'D'], ['E', 'F', 'G']],
    [['A', 'B', 'C', 'D'], ['E', 'F', 'G']]
]

# Calculate the TD^2 for each solution
for i, solution in enumerate(solutions):
    print(f"Solution {i+1}:")
    total_td2 = 0
    
    for cluster in solution:
        centroid = np.mean(df[df['ID'].isin(cluster)]['Value'])
        td2 = sum((df[df['ID'].isin(cluster)]['Value'] - centroid) ** 2)
        print(f"Cluster {cluster}: Centroid = {centroid:.2f}, TD^2 = {td2:.2f}")
        total_td2 += td2
        
    print(f"Total TD^2 = {total_td2:.2f}\n")
```

# Question 4 Simplified silhouette 

[[solution04.pdf#page=]]
We have the following one-dimensional dataset:

| ID | Value |
| --- | --- |
| A | 1 |
| B | 3 |
| C | 5 |
| D | 7 |
| E | 10 |
| F | 11 |
| G | 12 |
In three attempts, k-means delivered the following three clustering solutions:
$$
\begin{aligned}
& S_1=\{A, B, C\},\{D, E, F, G\} \\
& S_2=\{A, B\},\{C, D\},\{E, F, G\} \\
& S_3=\{A, B, C, D\},\{E, F, G\}
\end{aligned}
$$
We want to compare the solutions using simplified Silhouette. Which of the following statements are correct?
- [ ] $S_1$ is better then $S_2$ in terms of simplified silhouette
- [x] $S_2$ is better then $S_3$ in terms of simplified silhouette
- [ ] $S_1$ and $S_2$ are equally good in terms of simplified silhouette
- [x] $S_3$ is better then $S_1$ in terms of simplified silhouette

### Solutions



# Question 5 (EM-clustering)
EM-clustering: which of the following statements are true:

- [ ] EM clustering make us of Bayes'rule
- [ ] EM clustering assumes independence between attributes
- [ ] EM clustering is a generalization of k-means
- [ ] The principle of EM clustering is to find the MAP hypothesis

### Solution
- [x] EM clustering make us of Bayes'rule
- [ ] EM clustering assumes independence between attributes
- [x] EM clustering is a generalization of k-means
- [x] The principle of EM clustering is to find the MAP hypothesis


-    EM clustering make us of Bayes' rule: EM (Expectation-Maximization) clustering is a probabilistic method used for unsupervised learning, which means it aims to find patterns in data without any prior knowledge of the categories or labels. In this method, the probability distribution of the data is modeled using a mixture of probability distributions, each representing a cluster or category. The Bayes' rule is used to estimate the posterior probability of the latent variables (cluster membership) given the observed data.
    
-    EM clustering assumes independence between attributes: This statement is not true. EM clustering assumes that the data is generated by a mixture of probability distributions, but it does not assume independence between the attributes. In fact, the covariance matrix of each component in the mixture model can be different, indicating the presence of correlation between attributes.
    
-    EM clustering is a generalization of k-means: K-means is a clustering algorithm that partitions data into k clusters by minimizing the sum of squared distances between data points and their corresponding cluster centers. EM clustering is a generalization of k-means because it also partitions data into clusters, but it does not assume that the clusters are spherical or that the distances between data points and cluster centers are Euclidean. Instead, EM clustering models the data as a mixture of probability distributions, which can have different shapes and orientations in the feature space.
    
-    The principle of EM clustering is to find the MAP hypothesis: The principle of EM clustering is to find the Maximum A Posteriori (MAP) hypothesis, which represents the cluster assignment that maximizes the posterior probability given the observed data. The EM algorithm iteratively estimates the posterior distribution of the latent variables (cluster membership) using the observed data and the current estimate of the parameters (mixture weights, means, and covariance matrices), and then updates the parameters using the estimated posterior probabilities. The process continues until convergence, at which point the final parameter estimates represent the MAP hypothesis.
**EM Clustering**:
Expectation–maximization algorithm
**MAP:** Maximum A Posteriori

# Question 6
- [ ] The number of parameters to describe a d-dimensional normal distribution grows quadratically with the number of dimensions
- [ ] In 10-fold cross-validation, each object is used exactly ten times for testing
- [ ] A non-parametric clustering algorithm is an algorithm that does not require any user-specified parameters
- [ ] A large decision tree has higher tendency to overfit than a smaller decision tree.
- [ ] Neural Nets an decision Trees search heuristically for separation boundaries and cannot guarantee to find the best solution
### Solution
- [x] The number of parameters to describe a d-dimensional normal distribution grows quadratically with the number of dimensions
- [ ] In 10-told cross-validation, each object is used exactly ten times for testing
- [ ] A non-parametric clustering algorithm is an algorithm that does not require any user-specified parameters
- [x] A large decision tree has higher tendency to overfit than a smaller decision tree.
- [x] Neutral Nets an decision Trees search heuristically for separation boundaries and cannot guarantee to fin the best solution

# Question 7 k-means test k
Using Manhattan distance, for which choices of k would the kNN classifier classify the query point (triangle) as square?
![[Pasted image 20230325123317.png]]
- [ ] 3
- [ ] 6
- [ ] 10
- [ ] 13
- [ ] 15
- [ ] 17
- [ ] 18

### Solution
- [x] 3
- [ ] 6
- [ ] 10
- [ ] 13
- [ ] 15
- [x] 17
- [x] 18
Tegn løsningen og tæl
![[k=3.png]]
eller kør følgende python program
``` Python
import operator

# Define the datasets
circle = [(5,2), (6,1), (6,3), (6,4), (7,2), (8,2), (8,5), (9,3)]
square = [(2,8), (3,4), (4,4), (4,6), (4,8), (5,7), (6,6), (7,3), (7,5), (8,9)]
triangle = (7,4)

# Define the Manhattan distance function
def manhattan_distance(p1, p2):
    return abs(p1[0]-p2[0]) + abs(p1[1]-p2[1])

# Define the kNN classifier function
def knn_classifier(k, query_point, dataset):
    distances = []
    for data_point in dataset:
        dist = manhattan_distance(query_point, data_point)
        distances.append((data_point, dist))
    distances.sort(key=operator.itemgetter(1))
    k_nearest = distances[:k]
    class_count = {}
    for nearest in k_nearest:
        data_point = nearest[0]
        if data_point in square:
            class_label = 'square'
        elif data_point in circle:
            class_label = 'circle'
        else:
            class_label = 'unknown'
        class_count[class_label] = class_count.get(class_label, 0) + 1
    sorted_class_count = sorted(class_count.items(), key=operator.itemgetter(1), reverse=True)
    return sorted_class_count[0][0]

# Test the classifier for different values of k
for k in range(1, 20):
    predicted_class = knn_classifier(k, triangle, square+circle)
    print(f'For k={k}, the predicted class is {predicted_class}.')
```

# Question 8 Precision recall
Given the true class of 10 test objects and the predictions of some classifier $h$, which statements are correct w.r.t. to the class specific evaluation measures recall and precision?

| object 0 | true class (f(0)) | prediction (h(0)) |
| --- | --- | --- |
| $O_{(1)}$ | A | A |
| $O_{(2)}$ | A | A |
| $O_{(3)}$ | A | A |
| $O_{(4)}$ | A | B |
| $O_{(5)}$ | A | B |
| $O_{(5)}$ | A | A |
| $O_{(7)}$ | B | B |
| $O_{(8)}$ | B | B |
| $O_{(9)}$ | B | A |

- [ ] recall for class A $>$ recall for class B
- [ ] precision for class A $>$ precision for class B
- [ ] precision for class A $>$ recall for class B
- [ ] precision for class B $>$ precision for class B

### Solution
Vi starter med at lave en confusion matrix:

| 0   | A   | B   |
| --- | --- | --- |
| A   | 4   |  2   | 
| B   |  1   | 2   |

#### A
Truth positive: $1+1+1+1=4$
False positive:  $1$
True negative: $1+1=2$
False: negative $1+1=2$
Recall = $\frac{TP}{TP+FN}=\frac{4}{4+2}=\frac{4}{6}=\frac{2}{3}$
Precision $=\frac{TP}{TP+FP}=\frac{4}{5}=\frac{4}{5}$

#### B
Truth positive: $1+1=2$
False positive:  $1$
True negative: $1+1+1+1=4$
False: negative $1=1$
Recall = $\frac{TP}{TP+FN}=\frac{2}{2+1}=\frac{2}{3}$
Precision $=\frac{TP}{TP+FP}=\frac{2}{2+1}=\frac{2}{3}$

- [ ] recall for class A $>$ recall for class B
- [x] precision for class A $>$ precision for class B
- [x] precision for class A $>$ recall for class B
- [ ] precision for class B $>$ precision for class B

#### python
``` Python
from sklearn.metrics import confusion_matrix, classification_report

# true class of the test objects
y_true = ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'A']

# predicted class by the classifier
y_pred = ['A', 'A', 'A', 'B', 'B', 'A', 'B', 'B', 'A']

# calculate the confusion matrix
cm = confusion_matrix(y_true, y_pred)

# print the confusion matrix
print("Confusion matrix:")
print(cm)

# calculate the classification report which includes precision, recall, f1-score and support
cr = classification_report(y_true, y_pred)

# print the classification report
print("Classification report:")
print(cr)
```

``` Python
Confusion matrix:
[[4 2]
 [1 2]]
Classification report:
              precision    recall  f1-score   support

           A       0.80      0.67      0.73         6
           B       0.50      0.67      0.57         3

    accuracy                           0.67         9
   macro avg       0.65      0.67      0.65         9
weighted avg       0.70      0.67      0.68         9
```

# Question 9 bayes optimal classifier
We have a classification problem with two classes " + " and "-", three trained classifiers $h_1, h_2$, and $h_3$, with the following probabilities of the classifiers, given the training data $D$ :
$$
\begin{aligned}
& \operatorname{Pr}\left(h_1 \mid D\right)=0.4 \\
& \operatorname{Pr}\left(h_2 \mid D\right)=0.1 \\
& \operatorname{Pr}\left(h_3 \mid D\right)=0.5
\end{aligned}
$$
For the three test instances $\mathrm{O}_1, \mathrm{O}_2, \mathrm{O}_3$, the classifiers give the following class probabilities:
$$
\begin{aligned}
& o_1: \\
& \operatorname{Pr}\left(+\mid h_1\right)=0.7, \operatorname{Pr}\left(-\mid h_1\right)=0.3 \\
& \operatorname{Pr}\left(+\mid h_2\right)=0.2, \operatorname{Pr}\left(-\mid h_2\right)=0.8 \\
& \operatorname{Pr}\left(+\mid h_3\right)=0.5, \operatorname{Pr}\left(-\mid h_3\right)=0.5
\end{aligned}
$$
$$
\begin{aligned}
& o_2: \\
& \operatorname{Pr}\left(+\mid h_1\right)=0.8 \cdot \operatorname{Pr}\left(-\mid h_1\right)=0.2 \\
& \operatorname{Pr}\left(+\mid h_2\right)=0.6, \operatorname{Pr}\left(-\mid h_2\right)=0.4 \\
& \operatorname{Pr}\left(+\mid h_3\right)=0.5 \cdot \operatorname{Pr}\left(-\mid h_3\right)=0.5
\end{aligned}
$$
$$
\begin{aligned}
& 0_3: \\
& \operatorname{Pr}\left(+\mid h_1\right)=0.6, \operatorname{Pr}\left(-\mid h_1\right)=0.4 \\
& \operatorname{Pr}\left(+\mid h_2\right)=0.6 \cdot \operatorname{Pr}\left(-\mid h_2\right)=0.4 \\
& \operatorname{Pr}\left(+\mid h_3\right)=0.5, \operatorname{Pr}\left(-\mid h_3\right)=0.5
\end{aligned}
$$
We combine the three classifiers to get a Bayes optimal classifier. Which of the following class probabilities will we get from this Bayes optimal classifier?

Class probabilities for test instance O1:
+: 0.55
-: 0.45
Class probabilities for test instance O2:
+: 0.6300000000000001
-: 0.37
Class probabilities for test instance O3:
+: 0.55
-: 0.45000000000000007

### Python
``` Python
import numpy as np

# probabilities of classifiers given the training data
p_h1 = 0.4
p_h2 = 0.1
p_h3 = 0.5

# class probabilities for each test instance and classifier
o1_h1 = np.array([0.7, 0.3])
o1_h2 = np.array([0.2, 0.8])
o1_h3 = np.array([0.5, 0.5])

o2_h1 = np.array([0.8, 0.2])
o2_h2 = np.array([0.6, 0.4])
o2_h3 = np.array([0.5, 0.5])

o3_h1 = np.array([0.6, 0.4])
o3_h2 = np.array([0.6, 0.4])
o3_h3 = np.array([0.5, 0.5])

# combine the classifiers using Bayes optimal classifier
p_o1 = p_h1 * o1_h1 + p_h2 * o1_h2 + p_h3 * o1_h3
p_o2 = p_h1 * o2_h1 + p_h2 * o2_h2 + p_h3 * o2_h3
p_o3 = p_h1 * o3_h1 + p_h2 * o3_h2 + p_h3 * o3_h3

# print the results
print("Class probabilities for test instance O1:")
print("+:", p_o1[0])
print("-:", p_o1[1])

print("Class probabilities for test instance O2:")
print("+:", p_o2[0])
print("-:", p_o2[1])

print("Class probabilities for test instance O3:")
print("+:", p_o3[0])
print("-:", p_o3[1])
```

# Question 11 OPTICS
1. Is B since we see that B contains all of the other clusters
2. This has to be A as it is not contained in fx by B
3. This is D as it contains two clusters inside which we see
4. Must be C as it contains no no other clusters inside
5. E or F 
6. E or F

# 15 DBSCAN
**density-connected**: through path via others which you can find through density reachable
**density-reachable**: if you can reach it from the point itself

**minPts**: All points through the connection has to have these **minPts** wrt.  $\epsilon$

# 16
eyeball

# 17 ROC AUC
calculate the indexes and see which one is ranked better. Fx `m1` & `m2`. 
1. m1: A: 3, B: 6 = 9
2. m2: A: 2, 7 = 9
+ m3: A: 3, B: 7 = 10
+ m4: A: 4, B=5 = 9
**The lowest is better**

# 18 Perceptrons, decision trees, SVMs
1. False: Not so binary should not be. This would be multiple vandrette and lodrette lines
2. True: Perceptron is like a neural network, can learn functions
3. False: Does not maximize distance between points
4. False: just like 1
5. True: activation function which is just linear
6. True: maximize distance between the support vectors. Fx better than h3, since distance from the two support vectors **The ones which are closest** (3,4), (6,2) is maximized fx compared to h3
7. True binary split
8. True linear activation function
9. **False: Does not maximize the distance to each support vector. H2 could be a support vector**

