[[_exercise16.pdf]]
# Which of the following statements are true about the Expectation Maximization (EM) and k-means clustering algorithms?
#EM_exercises 
#K-means_exercises
Both for clustering. We know the number of K-clusters

EM each cluster has a mean and covariance-matrix. Each cluster should have the same mean and covariance-matrix

[[_DM566-2023-part5.pdf#page=70]]

[[7. Bayes Optimal Classifier, Naive Bayes, Random Variables and Distributions, EM Clustering]]


(a) The EM algorithm can learn covariances between feature dimensions while the k−mean algorithm cannot.

**True**, EM uses covariance and mean. K-means does not use covariance only distance.

(b) After being trained with two diﬀerent choices of k, the k-means algorithm can provide a preference score, while EM cannot.

preference score is the ranking, of each data point in the data-set. None of the k-means or EM uses preference score, since they dont rank the points. EM and k-means just clusters the points. 

**False**

(c) While it is possible to choose the optimal k after training EM once on a data set, it is not possible for k−means.

**False**, for noone of the algorithms we can find the best k. Fx could use silhouette score

(d) While the Mac-Queen version of the k−means output is dependent on the processing order of the data points, the EM output is not.
[[_DM566-2023-part2.pdf]]
#EM_exercises
**True**, K-means Mac-quuen is dependent on the ordering, EM is not. EM calculates for all points and then updates.

(e) While the Lloyd-Forgy version of the k−means output is dependent on the processing order of the data points, the EM output is not.

**False**, lloyd-forgy  calculates for all the points and then updates the clusters, therefore not processing order dependent. Em is still not dependent as it does the same

(f) When trained on the same cluster count k, Expectation Maximization has more parameters to ﬁt than k-means.

**True**, EM uses mean and covariance, k-means uses only the mean.

# 16-2
Activation function is the $\sigma$
#concave_convex
![[Pasted image 20230531130945.png]]

non-linear function = non-convex
linear function = convex

minus trekant finder lokalt minimum, gradient decent
plus trekant finder lokalt maksimum, gradient assend

Remember every global minimum is also a local minimum

a. **False**,  could also be finding the global maximum or local minimum, due to the fact that the loss function is non-linear.
b. **False**, we cannot be sure that it is global. The function is non-convex
c. **False**, We go to the direction of negative gradient, can find a local minimum
d. **False** can find a local maximum
e. **True** goes to negative gradient, i.e going downwards. 
f. **True**, just f(x)=x
g. **True**, Still linear meaning it is convex having only 1 local/global mean and maximum
h. **False** the activation function is non-linear i.e non-convex



# 16-3
#binary_classification_prediction_exercise
## Consider the binary classiﬁcation problem below
![[Pasted image 20230530131752.png]]
Which of the below classiﬁers have the potential to give zero prediction error ?
(a) Neural net with two hidden layers and activation function σ(u) = max(0, u).

function is just f(x)=x if x > 0 else 0. This is non-linear activation function. It can learn the non-linearity, i.e it can learn the complex patterns

**True**

(b) Neural net with seven hidden layers and activation function σ(u) = u.

**False**, the pattern cannot be learned by linear separation

(c) SVM with linear kernel

**False** can only learn linear pattern again

(d) SVM with squared exponential kernel

**True**, can learn non-linear patterns

(e) Linear regression

**False**, again a line

(f) Decision tree with axis-aligned splits max depth 2

First split fx x and then on y. Cannot do this if looking at the figure. Split on feature 2 or feature 1 first, and then choose the next 

Can also be explained by the fact that the problem is complex, and therefore a depth of 2 is probably not enough

**False**, 

(g) Decision tree with axis-aligned splits, unlimited max dept

**True**