# IMPORTANT
+ reducing k in k-means introduces more bias.


[[dm566-exam-spring2022.pdf]]
# 1
[[_DM566-2023-part1 (1).pdf#page=88]]
yes:
+ AD 
+ ADE
+ CD
+ ABD
+ DE


# 2
FIXME

# 3
[[16.]] has all the answers

# 4  k-means clustering (use python script, calculate the first points to be sure the script is modified correctly)
x1 & x2 is the first point, and y1 & y2 is the 2nd point
![[Pasted image 20230622091936.png]]

# 5 random variables
use script from tobias <3

# 6 bayes optimal classifier (script)
Use script, just calculate the possibilities against  all of the test instances
fx 

## $O_{1}$ +
$$0.4*0.5+0.2*0.6+0.4*0.3$$

1. False
2. True
3. true
4. true
5. false
6. false

# 7 overfit underfit
**underfit: overall shit. Bad train error and test error. Usually with low model of parameters**
1. 5 overfitted
2. false
3. true because better train error fx compared to model with less params
4. true
5. true
6. false
7. false, 3 is fx better

# 8 decision tree (see solution from tobias for information gain)
for X1=B we see that X2 has 2Ds which are both +1 meaning it has minimum impurity therefore it will chose X2

for X1=A we see that E is only -1 in X3 having minimum impurity
- False
- True
- True 
- False

# 9 naive bayes classifier
1. True 
2. False
3. true
4. False



## 1, (B,E)
### For +
caclulate for B:
+ 2x -
+ 1x +

calculate for E
+ 2x +
+ 1x -

how many +
+ 4/9

$$\frac{Pr(B|E|+)}{(Pr(B|E|+)+(Pr(B|E|-))}$$

calculate Pr(B|E|+):
$$\frac{\frac{1}{3}* \frac{2}{3} * \frac{4}{9}}{\left( \frac{1}{4} *  \frac{2}{4} * \frac{4}{9}\right)+\left( \frac{2}{3} * \frac{1}{3} * \frac{5}{9}\right)}$$
## 2. (C,E)
### For +
for C:
+ 2x -
+ 0x +
calculate for E
+ 2x +
+ 1x -
![[Pasted image 20230622170641.png]]
## 3. (A,D)
### D
+ 2 +
+ 4 -
### A
+ 3 +
+ 1 -
#### Possibility for +
$$\frac{\frac{2}{6}* \frac{3}{4} * pr(+1|Y)}{\left(\frac{2}{6}* \frac{3}{4} * pr(+1|Y)\right)+ \left( \frac{4}{6} * \frac{1}{4} * Pr(-1|Y)\right)}$$
![[Pasted image 20230622170902.png]]

## 4. (B,D)
B:
+ 2x -
+ 1x +
D:
+ 2x +
+ 4x -
![[Pasted image 20230622171106.png]]

# 10 precision recall (script)
True positive: It should predict B and it predicts B 
False negatives: It should predict B but predicts something else
False positive: Should not predict B since it is something else
$$true\ positive = \frac{TP}{TP+FP}$$
$$Recall=\frac{TP}{TP+FN}$$
1. False
2. tRUE
3. tRUE
4. TRUE
5. false
6. false
7. false


# 11 DBSCAN
**The point itself counts**, meaning if MinPts = 1 then every point is a core point
**manhatten distance** is the one with 4 triangles.

Edge point when you as a point dont have enough min-points, but you are included by another point which has enough points

1. True
2. false, it has 4
3. True, has 3
4. False, has 4
5. True, has many many
6. False, has 2
7. True
8. False


# 12 k-nearest neighbor
If k is less, then we have more bias towards the points. If k is large then bias is less

1. True, more points = less bias
2. False, training has nothing to do with the data. Only has K has this impact on the bias
3. False, increase bias, the points next to you speeks less
4. True, we test more points. Fx try k=2, k=3 ... then you have already tried more points
5. False, Only training set has impact to modify the model
6. True

# 13 . 
max(u,0) is non-linear meaning the graph is not convex [[16.]]

minus trekant finder lokalt minimum, gradient decent
plus trekant finder lokalt maksimum, gradient assend

1. False, PLUS trekant finds local maksimum  
2. False, the activation function is non-linear again.  I.e non-convex
3. True, linear function meaning we have convex graph, can find global minimum
4. True, non-linear activation function meaning we have a non-convex, i.e can only find local minimum, and have gradiant decent: **negative trekant**
5. True, convex since linear activation function, only have unique global.
6. False, can also be smaller than a global: still nonconvex function due to the activation function being non-linear
7. False, can maybe be local

# 14
1. False, cannot learn anything which is non-linear
2. False, linear kernel cannot learn anything which is non-linear
3. dont know
4. True, unlimited
5. False, limited
6. True, non-linear function
7. True, non-linear function 

# 15
After merging, before pruning (pruning then you would have to find in the items, where the combinations are in the set)
**MERGING**: find all possible combinations, and then check if all subsets are in the previous list

1. True, all subsets are in
2. False, CD not there
3. True
4. True
5. True
6. False, AD not there
7. False, BE not there
8. False, DE not there


# 16 SVM
The line will be at y=5

1. False
2. False
3. True, y = 5
4. False 
5. True
6. False
7. True 

# 17 k-Nearest neighbors
![[KNearest.excalidraw]]
1. True
2. True
3. True 
4. False
5. False