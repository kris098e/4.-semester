![[_DM566-2023-part6.pdf]]
# Keywords
## what is machine learning
sees data and learns from it.

## linear regression
Loss function, minimize this with some input
random shooting the value that will minimize the loss function, works incredibly well.
Minimum squared error

## Overfitting and Underfitting
Cannot focus on just the training case
make the gap between training and test error small
Optimal capacity, see graph picture[[_DM566-2023-part6.pdf#page=13]]

## Quantifying the optimism of training error
opitimism of the training error should have data set size in mind. The bigger the set, the better the optimism of how the training error/test error reflects the real world

look at when $N\to +inf$  [[_DM566-2023-part6.pdf#page=21]]

# recap
## supervised learning
* Linear regression
	* input gives output.
	* minimize mean squared error given some input
		* loss function
	* if cannot solve the problem via setting the gradient to 0 and solving, do gradient dissent. 

# Keywords
## Bias variance and noise in supervised learning
Look slide 30
[[_DM566-2023-part6.pdf#page=30]]

### Supervised learning as estimator inference
mapping to a y given some noise and input via some **f**

guess depends on population of data

loss is if guess is not correct

### Bias of an estimator

### standard error of the mean
better to give performance of a model in an interval: 
mean of performance +- mean squared error

### Bias introduction
as soon as we choose a model family, we introduce bias into the resulting model


## Ensembles
train multiple models, when a new data point comes, ask all the models and pick the models that performs the best, and then combine these

## Information Theory
the degree of suprise increases when the probability of this to event to happen lowers

### Entropy: The degree of surprise
calculation example on [[_DM566-2023-part6.pdf#page=40]]
the lover the $H[x]$ the less surprising the outcome is 

intution of entroypy **look at the note**[[_DM566-2023-part6.pdf#page=43]]
