![[_DM566-2023-part6.pdf]]
# Keywords
## what is machine learning
sees data and learns from it.

## linear regression
Loss function, minimize this with some input
random shooting the value that will minimize the loss function, works incredibly well.
Minimum squared error

## Overfitting and Underfitting
Cannot focus on just the training case. I.e use less training data.
make the gap between training and test error small
Optimal capacity, see graph picture[[_DM566-2023-part6.pdf#page=13]]

## Quantifying the optimism of training error
opitimism of the training error should have data set size in mind. The bigger the set, the better the optimism of how the training error/test error reflects the real world

look at when $N\to +inf$  [[_DM566-2023-part6.pdf#page=21]]

# recap
## supervised learning
* Linear regression
	* input gives output.
	* minimize mean squared error given some input
		* loss function
	* if cannot solve the problem via setting the gradient to 0 and solving, do gradient dissent. 
		* Faster to do the solving

# Keywords
## Bias variance and noise in supervised learning
Look slide 30
[[_DM566-2023-part6.pdf#page=30]]

### Supervised learning as estimator inference
mapping to a y given some noise and input via some **f**

guess depends on population of data

loss is if guess is not correct

### Bias of an estimator

### standard error of the mean
better to give performance of a model in an interval: 
mean of performance +- mean squared error

### Bias introduction
as soon as we choose a model family, we introduce bias into the resulting model


## Ensembles
train multiple models, when a new data point comes, ask all the models and pick the models that performs the best, and then combine these.

## Information Theory
the degree of surprise increases when the probability of this event happen lowers

### Entropy: The degree of surprise
calculation example on [[_DM566-2023-part6.pdf#page=40]]
the lover the $H[x]$ the less surprising the outcome is 

intution of entroypy **look at the note**[[_DM566-2023-part6.pdf#page=43]]
This means that when we do the calculation of the **expectation** we do take in consideration the values that x takes, but when calculating the entropy, we do not care about the values, only the probabilities.

## Convexity
put lambda = 1, and b will have no contribution. So this is how to go from point a to b, based on lambda

If condition shown holds, then convex. Else conway or can also be neither.

If the function is convex, it is enough to solve for only a single x value given the function $f(x)$, when solving for the derivative of _f_ . If neither conway or convex we will have to solve for $f'(x)=0$ and find the global minimum or maximum, depending on what we want to find.

### Jensens inequality

## KL divergence 
how similar two things are to each other

## Gini index
more impure dataset means a higher gini index. We will try to maximize the purity for the decision tree.

## Decision tree
Leafs are the end classification. nodes are based on a feature, and the 2 branches are then rules to go down the tree

Whitebox model: sees every single decision.  This is also why it is very much used, since we can see every single step. If using nerual nets, it is calculated by magic numbers which we cannot explain.

### Where to split
categorical attributes
numerical attributes

[[_DM566-2023-part6.pdf#page=56]]


#### Quality Measure: Information Gain
as randomness decreases, entropy decreases. This means that a better measure tries to minimize entropy.

Can also calculate this with Gini indix.

maximize information gain is best.

see example at
[[_DM566-2023-part6.pdf#page=61]]

## Axis-parallel hyperplanes
[[_DM566-2023-part6.pdf#page=67]]

**Sample complexity** might be very high, which we dont want. This means we need more data to make good model.

This will overfit if we try minimize training error, because we may end up with 50 criterias for why this specific customer purchased a milk

Dont want the decision tree to be very deep. So put threshold on depth.
Decide also level of impurity. Base it some threshold for purity.

### If not axis-parallel
w can take other values than [0,1] and [1,0] and we have to solve a linear regression for each gini-index, whic his also very computational heavy

### overfitting underfitting
[[_DM566-2023-part6.pdf#page=73]]

Herustics for not overfitting
[[_DM566-2023-part6.pdf#page=75]]

Error-reduction-pruning

If using the same test set twice, it is not statisticly allowed. It will then become more of a validation set.  Have to use a new test set each time. The solution to this becomes using a sufficiently large test set.

Having a larger tree means overfitting. However if we choose that we stop branching when impurity fx becomes > 90%, then we introduce bias into the model aswel. So when we prune a tree based on some critera to cause less test error / making it more generalized, then we introduce bias into the model.

# Most important concepts
1. Jensens inequality 
2. KL divergence viewed from statistic
3. Entropy
4. Information gain  (based on decision trees)
5. decision trees
6. axis-parralel hyperplanes

